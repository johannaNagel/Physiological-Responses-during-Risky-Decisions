{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01346031",
   "metadata": {},
   "source": [
    "# Statistical Tests\n",
    "\n",
    "This notbeook includes all statistical tests for the analysis.\n",
    "\n",
    "- Correlation Test\n",
    "- Mann-Whitney-U test\n",
    "- Wilcoxon\n",
    "- Linear Mixed Models (LMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  All imports\n",
    "import sys, glob, json, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import math\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import probplot\n",
    "import seaborn as sns\n",
    "import os\n",
    "import itertools\n",
    "#Imports for Predicition \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1958df",
   "metadata": {},
   "source": [
    "#### Loading data\n",
    "\n",
    "Loading csv data files with all necessary informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95149f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder with all results\n",
    "OUTDIR = Path(\"/Users/johannanagel/Masterarbeit_Programming/output_postfeedback\")\n",
    "#folder with the dataframes of the preparation files (market mechanism)\n",
    "RESULTS = Path(\"/Users/johannanagel/Masterarbeit_Programming/output_postfeedback/Main_Part/DataFrames\")\n",
    "STAT_READY = RESULTS / \"statistical_analysis_ready.csv\"\n",
    "SCHEMA = RESULTS /\"schema_results.json\"\n",
    "#folder for the results of the analyis\n",
    "TESTS_RESULT = Path(\"/Users/johannanagel/Masterarbeit_Programming/output_postfeedback/Main_Part/Statistical_Test\")\n",
    "os.makedirs(TESTS_RESULT, exist_ok = True)\n",
    "FULL_RESULTS =    RESULTS / \"all_data_combined.csv\"\n",
    "\n",
    "\n",
    "DEMOGRAPHICS = RESULTS/ \"demographics_participant.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "try: \n",
    "    #reading .csv file which represent the data which is ready for the analysis\n",
    "    df_all = pd.read_csv(STAT_READY)\n",
    "    df_demo = pd.read_csv(DEMOGRAPHICS)\n",
    "    df_full = pd.read_csv(FULL_RESULTS)\n",
    "\n",
    "    with open(SCHEMA) as f:\n",
    "        schema = json.load(f)\n",
    "\n",
    "    for c, meta in schema.items():\n",
    "        df_all[c] = pd.Categorical(df_all[c], categories=meta[\"categories\"], ordered=meta[\"ordered\"])\n",
    "\n",
    "    df_all.sort_values([\"session\", \"participant\", \"round\"], ascending = True)\n",
    "except ValueError as err:\n",
    "    print(f\"Something went wrong with reading the file: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67cb4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new data (forgot mispricing data) to the analysis frame\n",
    "df_mispricing = df_full.copy()\n",
    "df_mispricing = df_mispricing[[\"session\",\"round\", \"rd_Smith_categorization\", \"Category_SMITH\", \"Category_Guisty\", \"Category_KIRCHLER\", \"Category_Guisty_round\", \"rd_Kirchler_categorization\"]].drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_all,df_mispricing, on = [\"session\", \"round\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for nan values \n",
    "print(\"Columns with nan values: \")\n",
    "df_all.isna().sum()[df_all.isna().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0cec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check mumber of categories per window\n",
    "df_all[\"window_type\"] = df_all[\"window_type\"].fillna(\"none\")\n",
    "df_all[\"window_type\"].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ac96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check datatypes\n",
    "print(\"Checking datatypes\")\n",
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e00ae",
   "metadata": {},
   "source": [
    "## Descriptive Analysis\n",
    "\n",
    "This chapter includes discriptive analysis steps (such as Shapiro nomarlization, or data summary of each column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ee857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables with columns based on market mechanisms and EDA\n",
    "GENERAL_COL = [\"session\", \"round\", \"participant\", \"price\", \"dividend\", \"interest\", \"stocks\"]\n",
    "EDA_COLUMNS = [\"window_type\", \"scl_mean\", \"scl_std\", \"scr_auc\", \"scr_amplitude\", \"scr_per_sec\", \"scr_count\", \"signal_range\", \"signal_std\"]\n",
    "FV_COL = [\"FV_KIRCHLER\", \"FV_GUISTY\", \"FV_SMITH\", \"rd_Guisty\", \"rd_Smith\", \"rd_Kirchler\", \"rad_Guisty\", \"rad_Smith\", \"rad_Kirchler\", \"rd_GUISTY_per_round\", \"rd_Smith_per_round\", \"rd_Kirchler_per_round\", \"rad_GUISTY_per_round\", \"rad_Smith_per_round\", \"rad_Kirchler_per_round\"]\n",
    "VOL_COL = [\"V_t\", \"MAD_t\", \"AMAD_t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating number of participants per session and the number of observations\n",
    "session_overview = df_all.groupby(\"session\")[\"participant\"].nunique().reset_index().copy()\n",
    "\n",
    "session_overview.columns = [\"session\", \"Number_participant\"]\n",
    "obs_per_session = df_all.groupby(\"session\").size().reset_index(name=\"Number_observations\").copy()\n",
    "\n",
    "# grouping number of unique participants\n",
    "session_stats = (\n",
    "    df_all.groupby(\"session\")\n",
    "      .agg(\n",
    "          Number_participant=(\"participant\", \"nunique\"),\n",
    "          Number_observations=(\"participant\", \"count\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "numb_session = df_all.session.nunique()\n",
    "numb_participant = df_all.participant.nunique()\n",
    "number_observations = len(df_all)\n",
    "print(number_observations)\n",
    "\n",
    "number = pd.DataFrame({\"session\": [f\"Total: {numb_session}\"] , \"Number_participant\": numb_participant, \"Number_observations\": number_observations})\n",
    "\n",
    "# Saving the dataframe in a latex table\n",
    "session_stats = pd.concat([session_stats, number], ignore_index=True)\n",
    "path = TESTS_RESULT / \"session_overview.tex\"\n",
    "session_stats.to_latex(\n",
    "    path,\n",
    "    index=False,\n",
    "    caption=\"Session Overview with number of paricipants\",\n",
    "    label=\"tab:session_overview\"\n",
    ")\n",
    "print(f\"saved in: {path}\")\n",
    "session_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe data values of the general columns\n",
    "stat_general = df_all[GENERAL_COL].describe().T\n",
    "stat_general = stat_general.round(3)\n",
    "\n",
    "stat_general = stat_general.reset_index()\n",
    "path = TESTS_RESULT / \"Description_General_Variables.tex\"\n",
    "df_clean = stat_general.astype(str)\n",
    "# Saving as a latex file\n",
    "df_clean.to_latex(\n",
    "    path,\n",
    "    index=False,\n",
    "    caption=\"Description_General_Variables\",\n",
    "    label=\"tab:description_General_Variables\"\n",
    ")\n",
    "\n",
    "print(f\"saved in {path}\")\n",
    "\n",
    "stat_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe data values of the EDA columns\n",
    "stats_EDA = df_all[EDA_COLUMNS].describe().T.round(3)\n",
    "\n",
    "path = TESTS_RESULT / \"Description_EDA.tex\"\n",
    "df_clean_eda = stats_EDA.astype(str)\n",
    "# Saving table\n",
    "df_clean_eda.to_latex(\n",
    "    path,\n",
    "    index=False,\n",
    "    caption=\"Description EDA Variables\",\n",
    "    label=\"tab:description_EDA\"\n",
    ")\n",
    "\n",
    "print(f\"saved in {path}\")\n",
    "stats_EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01424cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe data values of the volatility columns\n",
    "stats_VOL = df_all[VOL_COL].describe().T.round(3)\n",
    "path = TESTS_RESULT / \"Description_VOL.tex\"\n",
    "df_clean_VOL = stats_VOL.astype(str)\n",
    "\n",
    "# Saving latex file\n",
    "df_clean_VOL.to_latex(\n",
    "    path,\n",
    "    index=False,\n",
    "    caption=\"Description VOL Variables\",\n",
    "    label=\"tab:description_VOL\"\n",
    ")\n",
    "\n",
    "print(f\"saved in {path}\")\n",
    "\n",
    "stats_VOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe data values of the FV columns\n",
    "stats_FV = df_all[FV_COL].describe().T.round(3)\n",
    "\n",
    "path = TESTS_RESULT / \"Description_FV.tex\"\n",
    "df_clean_FV = stats_FV.astype(str)\n",
    "df_clean_FV.to_latex(\n",
    "    path,\n",
    "    index=False,\n",
    "    caption=\"Description FV Variables\",\n",
    "    label=\"tab:description_FV\"\n",
    ")\n",
    "\n",
    "print(f\"saved in {path}\")\n",
    "stats_FV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop nan values\n",
    "df_all = df_all.dropna(subset=[\"V_t\", \"MAD_t\", \"AMAD_t\", \"Vol_Classification_Global\", \"Vol_Classification_between_session\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a1146c",
   "metadata": {},
   "source": [
    "### Testing for Normal distribution of Attributes with Shapiro\n",
    "\n",
    "H0: The data is normal distributed\n",
    "<br>H1: Data is not normal distributed\n",
    "\n",
    "- if p < 0.05: Reject H0 (data is not normal distributed)  => Support H1\n",
    "- if p >= 0.05: No rejection of HO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccbf75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the columns of the data in numeric columns and categorical columns\n",
    "#only colums with numeric values\n",
    "numeric_cols = df_all.select_dtypes(include=[np.number]).columns.tolist()\n",
    "#categorical variables\n",
    "categorical_cols = df_all.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(\"\\nNumerical Cols:\")\n",
    "print(numeric_cols)\n",
    "\n",
    "print(\"\\nCategorical Cols:\")\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_shapiro(x):\n",
    "    \"\"\"Shapiro function for testing normal distribution \"\"\"\n",
    "    x = pd.to_numeric(x, errors=\"coerce\").dropna()\n",
    "    if len(x) < 3 or np.isclose(x.std(), 0):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return shapiro(x)[1]\n",
    "    except Exception:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ef7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply for each numeric columns the shapiro function\n",
    "try: \n",
    "  results = []\n",
    "  for column in numeric_cols:\n",
    "    if column in [\"participant\", \"session\", \"round\"] :\n",
    "      continue\n",
    "    else: \n",
    "      #apply it for each session\n",
    "      result = (\n",
    "          df_all.groupby([ \"session\"])[column]\n",
    "            .apply(safe_shapiro)\n",
    "            .reset_index(name=\"p_value\")\n",
    "      )\n",
    "      result[\"variable\"] = column\n",
    "      results.append(result)\n",
    "  result_normal_distributed_shapiro_session = pd.concat(results, ignore_index=True)\n",
    "  result_normal_distributed_shapiro_session = result_normal_distributed_shapiro_session[[ \"session\", \"variable\", \"p_value\"]]\n",
    "\n",
    "  table_shapiro_session = result_normal_distributed_shapiro_session.pivot(index=\"session\", columns=\"variable\", values=\"p_value\")\n",
    "  table_shapiro_session = table_shapiro_session.sort_index()\n",
    "  \n",
    "\n",
    "\n",
    "  #### check if the results include values with p> 0.05\n",
    "  \n",
    "\n",
    "  display(table_shapiro_session)\n",
    "  print(f\"If participants have normal distributed values: \\n\")\n",
    "  h0_no_rejection_session = result_normal_distributed_shapiro_session[result_normal_distributed_shapiro_session[\"p_value\"] >= 0.05]\n",
    "  print(h0_no_rejection_session)\n",
    "\n",
    "except ValueError as err:\n",
    "  print(f\"Something went wrong during the test of shapiro {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9103e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_normal_distributed_shapiro_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "  results = []\n",
    "  for column in numeric_cols:\n",
    "    if column in [\"participant\", \"session\", \"round\"] :\n",
    "      continue\n",
    "    else: \n",
    "      #apply for each participant\n",
    "      result = (\n",
    "          df_all.groupby([\"participant\", \"session\"])[column]\n",
    "            .apply(safe_shapiro)\n",
    "            .reset_index(name=\"p_value\")\n",
    "      )\n",
    "      result[\"variable\"] = column\n",
    "      results.append(result)\n",
    "  result_normal_distributed_shapiro = pd.concat(results, ignore_index=True)\n",
    "  result_normal_distributed_shapiro = result_normal_distributed_shapiro[[\"participant\", \"session\", \"variable\", \"p_value\"]]\n",
    "  table_shapiro_session = result_normal_distributed_shapiro.pivot(index=\"participant\", columns=\"variable\", values=\"p_value\").round(3)\n",
    "  display(table_shapiro_session)\n",
    "\n",
    "\n",
    "  #### check if the results include values with p> 0.05\n",
    "  h0_no_rejection = result_normal_distributed_shapiro[result_normal_distributed_shapiro[\"p_value\"] >= 0.05]\n",
    "  print(f\"If participants have normal distributed values: \\n{h0_no_rejection}\")\n",
    "\n",
    "except ValueError as err:\n",
    "  print(f\"Something went wrong during the test of shapiro {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving normal distributed variables as a latex file\n",
    "\n",
    "path_latex_shapiro = TESTS_RESULT / \"normal_distributed_varibales.tex\"\n",
    "latex_str = h0_no_rejection.to_latex(index = False, caption = \"Shaprio Test - No rejection possible\", label = \"tab:shapiro_normal_dis\", float_format = \"%.3f\")\n",
    "with open(path_latex_shapiro, \"w\", encoding= \"utf-8\") as f:\n",
    "    f.write(latex_str)\n",
    "\n",
    "if path_latex_shapiro.exists:\n",
    "    print(f\"saved in {path_latex_shapiro}\")\n",
    "else:\n",
    "        print(\"‚ùå file was NOT saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8642141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting Histogram and Q-Q Plot for assessing the normal distribution\n",
    "for idx, row in h0_no_rejection.iterrows():\n",
    "    name = row[\"participant\"]\n",
    "    var = row[\"variable\"]\n",
    "    fig, axes = plt.subplots(1,2, figsize=(8,4))\n",
    "    fig.suptitle(f\"{name} ‚Äî {var}\", fontsize=12, fontweight=\"bold\")\n",
    "    sns.histplot(df_all[df_all[\"participant\"] ==name][var], kde=True, ax=axes[0], color=\"skyblue\")\n",
    "    axes[0].set_title(\"Histogramm\", fontsize=12)\n",
    "    axes[0].set_xlabel(var)\n",
    "    axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    probplot(df_all[df_all[\"participant\"] ==name][var].dropna(), dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title(\"Q‚ÄìQ-Plot\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    path = TESTS_RESULT / f\"histogram_qq_{name}.png\"\n",
    "    fig.savefig(path)\n",
    "\n",
    "    if path.exists():\n",
    "        print(f\"saved in {path}\")\n",
    "    else:\n",
    "        print(\"‚ùå file was NOT saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de855c",
   "metadata": {},
   "source": [
    "Result of the the Spapiro test => the data is not normal distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae3241",
   "metadata": {},
   "source": [
    "### Correlation - Heatmap (global, session, participant)\n",
    "\n",
    "Plotting the correlation heatmap with Spearman correlation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f02df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df_all.select_dtypes(include=[\"number\"]).columns.drop(\n",
    "    [\"interest\", \"dividend\", \"FV_SMITH\", \"rd_Kirchler\", \"rd_Guisty\", \"rd_Smith\",\"rad_Kirchler\", \"rad_Guisty\", \"rad_Smith\"]\n",
    ")\n",
    "\n",
    "corr_spearman = df_all[numeric_cols].corr(method=\"spearman\")\n",
    "\n",
    "# Heatmap of the correlation (global)\n",
    "fig = plt.figure(figsize = (len(numeric_cols)*0.6, len(numeric_cols)*0.6))\n",
    "sns.heatmap(corr_spearman,  cmap=\"coolwarm\", center=0, annot = True, fmt = \".2f\",linewidths=0.5, square=True, cbar_kws={\"shrink\": 0.7} )\n",
    "plt.title(\"Spearman Correlation Matrix\", fontsize=16, pad=12, fontweight=\"bold\")\n",
    "\n",
    "#Saving heatmao as a figure \n",
    "FOLDER_HEATMAP_CORRELATION = TESTS_RESULT / \"HEATMAP_CORRELATION\"\n",
    "os.makedirs(FOLDER_HEATMAP_CORRELATION, exist_ok=True)\n",
    "\n",
    "save_path = FOLDER_HEATMAP_CORRELATION / \"heatmap_corr_global.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"‚úÖ plot was saved under: {save_path}\")\n",
    "else: \n",
    "    print(f\"‚ùå plot was not saved\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation session-wise and participant wise\n",
    "for group in [\"session\", \"participant\"]:\n",
    "    print(f\"Correlation within {group}\")\n",
    "    for pid, df_p in df_all.groupby(group):\n",
    "        numeric_cols = df_all.select_dtypes(include=[\"number\"]).columns.drop([\"interest\", \"dividend\", \"FV_SMITH\", \"rd_Kirchler\", \"rd_Guisty\", \"rd_Smith\",\"rad_Kirchler\", \"rad_Guisty\", \"rad_Smith\", \"final_earnings_mean\"])\n",
    "        corr_spearman = df_p[numeric_cols].corr(method=\"spearman\")\n",
    "\n",
    "\n",
    "        plt.figure(figsize = (len(numeric_cols)*0.6, len(numeric_cols)*0.6))\n",
    "        sns.heatmap(corr_spearman,  cmap=\"coolwarm\", center=0, annot = True, fmt = \".2f\",linewidths=0.5, square=True, cbar_kws={\"shrink\": 0.7} )\n",
    "        plt.title(f\"Spearman Correlation Matrix - {pid}\")\n",
    "        \n",
    "        #saving heatmap\n",
    "        save_path = FOLDER_HEATMAP_CORRELATION / f\"heatmap_corr_{pid}.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"‚úÖ plot was saved under: {save_path}\")\n",
    "        else: \n",
    "            print(f\"‚ùå plot was not saved\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f4450",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "Applying Spearman corrlation function to test whether there is a relationship between EDA signals and the market mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676572bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(\n",
    "    df, market_mechanism,\n",
    "    #eda_signals=(\"scl_mean\", \"scl_std\", \"scr_amplitude\", \"scr_auc\",\n",
    "                 #\"scr_count\", \"scr_per_sec\", \"signal_range\", \"signal_std\"),\n",
    "\n",
    "    eda_signals=( \"scr_amplitude\", \"scr_auc\", \"scr_count\", \"scr_per_sec\", \"scl_mean\", \"scl_std\"),\n",
    "    methods=(\"pearsonr\", \"spearmanr\"),\n",
    "    groupby_cols=None,       # e.g. None, [\"window_type\"], [\"session\"], [\"session\",\"window_type\"]\n",
    "    min_n=3,\n",
    "    min_std=1e-8,\n",
    "    coerce_numeric=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute correlations between FV deviation variables and EDA signals.\n",
    "    Optionally group by one or more columns (e.g., session, window_type).\n",
    "    Returns a tidy pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Check if all variables exist and are numeric\n",
    "    if coerce_numeric:\n",
    "        for col in set(market_mechanism) | set(eda_signals):\n",
    "            if col not in df.columns:\n",
    "                raise KeyError(f\"Missing column: {col}\")\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Check grouping columns \n",
    "    if not groupby_cols:\n",
    "        groups = [((), df)]  # treat all data as one group\n",
    "    else:\n",
    "        for g in groupby_cols:\n",
    "            if g not in df.columns:\n",
    "                raise KeyError(f\"Grouping column missing: {g}\")\n",
    "        groups = df.groupby(groupby_cols, dropna=False)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Adding the results to the results list\n",
    "    def _add_row(group_dict,key, mm, eda, r_spearman, p_spearman, r_pearson, p_pearson, n, note, sig_spearman, sig_pearson ):\n",
    "        row = dict(group_dict)\n",
    "        row.update({\n",
    "            \"key\": key,\n",
    "            \"market_mechanism\": mm,\n",
    "            \"EDA_signal\": eda,\n",
    "            \"correlation_coefficient_Spearman\": r_spearman,\n",
    "            \"p_value_Spearman\": p_spearman,\n",
    "            \"correlation_coefficient_Pearson\": r_pearson,\n",
    "            \"p_value_Pearson\": p_pearson,\n",
    "            \"N\": n,\n",
    "            \"note\": note,\n",
    "            \"significance_Spearman\": sig_spearman,\n",
    "            \"significance_Pearson\": sig_pearson\n",
    "        })\n",
    "        results.append(row)\n",
    "\n",
    "    # Iterate over groups (e.g. per window_type or session) ---\n",
    "    for key, df_g in groups:\n",
    "        if not groupby_cols:\n",
    "            group_dict = {}\n",
    "        else:\n",
    "            if not isinstance(key, tuple):\n",
    "                key = (key,)\n",
    "            group_dict = dict(zip(groupby_cols, key))\n",
    "\n",
    "        for mm in market_mechanism:\n",
    "            for eda in eda_signals:\n",
    "                pair = df_g[[eda, mm]].dropna()\n",
    "                n = len(pair)\n",
    "\n",
    "                # Skip if not enough data\n",
    "                if n < min_n:\n",
    "                    _add_row(group_dict,key, mm, eda, None, None, None, None, n, \"N < min_n ‚Äî skipped\", \"not significant\", \"not significant\")\n",
    "                    continue\n",
    "\n",
    "                x, y = pair[eda].values, pair[mm].values\n",
    "\n",
    "                # Skip if nearly constant\n",
    "                if np.std(x) < min_std or np.std(y) < min_std:\n",
    "                    _add_row(group_dict,key, mm, eda, None, None, None, None, n, \"Variance too small ‚Äî skipped\", \"not significant\", \"not significant\")\n",
    "                    continue\n",
    "\n",
    "                # Compute correlation(s)\n",
    "\n",
    "                try:\n",
    "                    r_pearson, p_pearson = pearsonr(x, y)\n",
    "                    r_spearman, p_spearman = spearmanr(x, y)\n",
    "                    sig_pearson = \"significant\" if (p_pearson is not None and p_pearson < 0.05) else \"not significant\"\n",
    "                    sig_spearman = \"significant\" if (p_spearman is not None and p_spearman < 0.05) else \"not significant\"\n",
    "                    _add_row(group_dict,key, mm, eda, r_spearman,p_spearman, r_pearson, p_pearson, n, None, sig_spearman, sig_pearson)\n",
    "                except Exception as e:\n",
    "                    _add_row(group_dict,key, mm, eda, None, None, None, None, n, f\"error: {e}\", \"not significant\", \"not significant\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Filter dataframes groups\n",
    "\n",
    "def filter_results(df_corr, groupby_cols, alpha = 0.05): \n",
    "    \"\"\"\n",
    "    Filtering the result dataframe for correlations and divide them into groups.\n",
    "    \"\"\"  \n",
    "    print(f\"\\nüöÄ Start: Filter correlation results - Group = {groupby_cols}\")\n",
    "    df = df_corr.copy()\n",
    "    correlation = df[\"correlation_coefficient_Spearman\"]  \n",
    "    p_values =   df[\"p_value_Spearman\"] \n",
    "    #dividing based on the correlation values into groups (low, medium, strong, not significant)\n",
    "    sig= p_values.le(alpha)\n",
    "    strong_pos =  sig & correlation.ge(0.5)     \n",
    "    medium_pos =  sig & correlation.ge(0.3)  &  correlation.lt(0.5)\n",
    "    small_pos = sig & correlation.ge(0.1) &  correlation.lt(0.3)\n",
    "    strong_neg =  sig & correlation.le(-0.5)\n",
    "    medium_neg = sig & correlation.le(-0.3) & correlation.gt(-0.5)\n",
    "    low_sig = sig & correlation.le(-0.1) & correlation.gt(-0.3) \n",
    "    only_sig = sig &  ~(strong_pos | medium_pos | small_pos | strong_neg | medium_neg | low_sig)\n",
    "    not_sig = ~sig\n",
    "\n",
    "    results_df = []\n",
    "    results_df.append({\"Case\": f\"r ‚â• 0.5\", \"Frequency\": strong_pos.sum()})\n",
    "    print(f\"Size r ‚â• 0.5 & p‚â§{alpha}: {strong_pos.sum()}\")\n",
    "\n",
    "    results_df.append({\"Case\": f\"0.3 ‚â§ r < 0.5\", \"Frequency\": medium_pos.sum()})\n",
    "    print(f\"Size 0.3 ‚â§ r < 0.5 & p‚â§{alpha}: {medium_pos.sum()}\")\n",
    "\n",
    "    results_df.append({\"Case\": f\"0.1 ‚â§ r < 0.3\", \"Frequency\": small_pos.sum()})\n",
    "    print(f\"Size 0.1 ‚â§ r < 0.3 & p‚â§{alpha}: {small_pos.sum()}\")\n",
    "\n",
    "    results_df.append({\"Case\": f\"r ‚â§ -0.5\", \"Frequency\": strong_neg.sum()})\n",
    "    print(f\"Size r ‚â§ -0.5 & p‚â§{alpha}: {strong_neg.sum()}\")\n",
    "\n",
    "    results_df.append({\"Case\": \"-0.5 < r ‚â§ -0.3\", \"Frequency\": medium_neg.sum()})\n",
    "    print(f\"Size -0.5 < r ‚â§ -0.3 & p‚â§{alpha}: {medium_neg.sum()}\")\n",
    "\n",
    "    results_df.append({\"Case\": f\"-0.3 < r ‚â§ -0.1\", \"Frequency\": low_sig.sum()})\n",
    "    print(f\"Size -0.3 < r ‚â§ -0.1 & p‚â§{alpha}: {low_sig.sum()}\")\n",
    "\n",
    "    results_df.append({\"Case\": f\"|r|<0.1\", \"Frequency\": only_sig.sum()})\n",
    "    print(f\"Size p‚â§{alpha} but |r|<0.1: {only_sig.sum()}\")\n",
    "\n",
    "    results_df.append({\"Case\": f\"p>{alpha}\", \"Frequency\": not_sig.sum()})\n",
    "    print(f\"Size p>{alpha} (not significant): {not_sig.sum()}\")\n",
    "\n",
    "\n",
    "    #Max and Min of significant values\n",
    "    if sig.any():\n",
    "        idx_max = correlation[sig].idxmax()\n",
    "        idx_min = correlation[sig].idxmin()\n",
    "        r_max, p_max = correlation.loc[idx_max], p_values.loc[idx_max]\n",
    "        r_min, p_min = correlation.loc[idx_min], p_values.loc[idx_min]\n",
    "        print(f\"max correlation: {r_max}, p: {p_max}, key: {df.loc[idx_max,'key']}, \"\n",
    "              f\"EDA: {df.loc[idx_max,'EDA_signal']}, mech: {df.loc[idx_max,'market_mechanism']}\")\n",
    "        print(f\"min correlation: {r_min}, p: {p_min}, key: {df.loc[idx_min,'key']}, \"\n",
    "              f\"EDA: {df.loc[idx_min,'EDA_signal']}, mech: {df.loc[idx_min,'market_mechanism']}\")\n",
    "    print(\"üèÅ Done\")\n",
    "\n",
    "    strong_or_weak = df[strong_pos | medium_pos | small_pos | strong_neg | medium_neg | low_sig]\n",
    "    only_significant = df[only_sig]\n",
    "    not_significant  = df[not_sig]\n",
    "\n",
    "    return strong_or_weak, only_significant, not_significant, pd.DataFrame(results_df)\n",
    "\n",
    "def combine_frequency(df_frequency):\n",
    "    \"\"\"\n",
    "    Calculating for each category the frequency. Aim: To get a better overview over the results. \n",
    "    \"\"\"\n",
    "    df = df_frequency.copy()\n",
    "\n",
    "    wide = df.pivot_table(index=\"Case\", columns=\"Scenario\", values=\"Frequency\", aggfunc=\"sum\", fill_value=0)\n",
    "\n",
    "\n",
    "    scenario_order = [ \"General\", \"Decision-Phase\", \"Transaction\", \"Session\", \"Session and Decision-Phase\", \"Performance\" ] \n",
    "    wide = wide.reindex(columns=[c for c in scenario_order if c in wide.columns])\n",
    "    case_order_row = [\n",
    "    \"r ‚â• 0.5\",\n",
    "    \"0.3 ‚â§ r < 0.5\",\n",
    "    \"0.1 ‚â§ r < 0.3\",\n",
    "    \"r ‚â§ -0.5\",\n",
    "    \"-0.5 < r ‚â§ -0.3\",\n",
    "    \"-0.3 < r ‚â§ -0.1\",\n",
    "    \"|r|<0.1\",\n",
    "    \"p>0.05\",\n",
    "    \"Total\"  \n",
    "]\n",
    "    # Keep order\n",
    "    ordered = [r for r in case_order_row if r in wide.index]\n",
    "    #print(ordered)\n",
    "    rest = [r for r in wide.index if r not in set(ordered)]\n",
    "    #print(rest)\n",
    "    wide = wide.reindex(index=ordered + rest)\n",
    "\n",
    "\n",
    "    #calc total of categeories for each sceanrio\n",
    "    col_totals = wide.sum(axis=0, numeric_only=True)         \n",
    "    wide = pd.concat([wide, pd.DataFrame([col_totals], index=[\"Total\"])])\n",
    "\n",
    "    #calc relative frequency\n",
    "    denom = col_totals.replace(0, np.nan)\n",
    "    wide_rel = wide.div(denom, axis=1) * 100\n",
    "    wide_rel = wide_rel.fillna(0)\n",
    "    wide_rel.loc[\"Total\"] = 100.0\n",
    "\n",
    " \n",
    "    wide = wide.round(2).reset_index().rename(columns={\"index\": \"Case\"})\n",
    "    wide_rel = wide_rel.round(1).reset_index().rename(columns={\"index\": \"Case\"})\n",
    "\n",
    "    return wide, wide_rel\n",
    "\n",
    "def significance_marks(p):\n",
    "    \"\"\"\n",
    "        Marks for significance levels\n",
    "    \"\"\"\n",
    "    if pd.isna(p): return \"\"\n",
    "    elif p < 0.001: return \"***\"\n",
    "    elif p < 0.01: return \"**\"\n",
    "    elif p < 0.05: return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "def correlation_matrix_with_significance(\n",
    "    df_corr: pd.DataFrame,\n",
    "    *,\n",
    "    r_col=\"correlation_coefficient_Spearman\",\n",
    "    p_col=\"p_value_Spearman\",\n",
    "    mm_col=\"market_mechanism\",\n",
    "    eda_col=\"EDA_signal\",\n",
    "    alpha_levels=(0.05, 0.01, 0.001),\n",
    "    digits_r=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Creating a matrix of the correlation-values\n",
    "    \"\"\"\n",
    "\n",
    "    df = df_corr.copy()\n",
    "    df[\"cell_value\"] = df.apply(lambda x: f\"{x[r_col]:.{digits_r}f}{significance_marks(x[p_col])}\" if pd.notna(x[r_col]) else \"‚Äì\", axis=1)\n",
    "    #pivot the table for creating a matrix based on correlation effects and significants amrks\n",
    "    mat = df.pivot_table(index=mm_col, columns=eda_col, values=\"cell_value\", aggfunc=lambda x: x.iloc[0] if len(x) else \"-\")\n",
    "    if not mat.empty:\n",
    "        mat = mat.reindex(sorted(mat.columns), axis=1).fillna(\"-\")\n",
    "    return mat\n",
    "\n",
    "def all_scenario_matrices(\n",
    "    df_corr: pd.DataFrame,\n",
    "    group_cols,                              \n",
    "    *,\n",
    "    r_col=\"correlation_coefficient_Spearman\",\n",
    "    p_col=\"p_value_Spearman\",\n",
    "    mm_col=\"market_mechanism\",\n",
    "    eda_col=\"EDA_signal\",\n",
    "    alpha_levels=(0.05, 0.01, 0.001),\n",
    "    digits_r=2,\n",
    "    safe_names=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Applying correlation_matrix_with_significance method for each category of a column\n",
    "    \"\"\"\n",
    "\n",
    "    if not group_cols:\n",
    "        raise ValueError(\"Bitte group_cols angeben (z.B. ['window_type']).\")\n",
    "\n",
    "    out = {}\n",
    "    for key, g in df_corr.groupby(group_cols, dropna=False):\n",
    "        \n",
    "        if not isinstance(key, tuple):\n",
    "            key = (key,)\n",
    "        scen_name = \" √ó \".join(f\"{c}={v}\" for c, v in zip(group_cols, key))\n",
    "        if safe_names:\n",
    "            \n",
    "            scen_key = \"_\".join(f\"{c}-{str(v)}\" for c, v in zip(group_cols, key)).replace(\" \", \"_\")\n",
    "        else:\n",
    "            scen_key = scen_name\n",
    "\n",
    "        mat = correlation_matrix_with_significance(\n",
    "            g, r_col=\"correlation_coefficient_Spearman\", p_col=\"p_value_Spearman\", mm_col=mm_col, eda_col=eda_col,\n",
    "            alpha_levels=alpha_levels, digits_r=digits_r\n",
    "        )\n",
    "        out[scen_key] = mat\n",
    "    return out\n",
    "\n",
    "def calc_and_save_matrices(results_df, group_cols, folder):\n",
    "    \"\"\"\n",
    "    Executing the method for creating a matrix out of the r-values of the scenarios. And saving them as .tex file and .csv file\n",
    "    \"\"\"\n",
    "    \n",
    "    if results_df is None:\n",
    "        raise ValueError(\"No available results\")\n",
    "\n",
    "    if (group_cols == \"General\") | (group_cols == \"Performance\") :\n",
    "        matr = correlation_matrix_with_significance(results_df)\n",
    "        save_pdf_tex = folder / f\"corr_matrix_{group_cols}.tex\"\n",
    "        save_pdf_csv = folder / f\"corr_matrix_{group_cols}.csv\"\n",
    "        matr = matr.reset_index()\n",
    "\n",
    "        #saving as .csv file\n",
    "        matr.to_csv(save_pdf_csv, index=False, encoding=\"utf-8\", sep=\";\")\n",
    "        #saving as a latex file\n",
    "        with open(save_pdf_tex, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(matr.to_latex(\n",
    "                    index=False,\n",
    "                    escape=True,\n",
    "                    float_format=\"%.0f\",  \n",
    "                    caption=f\"Correlation Matrix FV vs EDA - {group_cols}\",\n",
    "                    label=f\"tab:Corr_Matrix_FV_EDA_{group_cols}\",\n",
    "                    column_format=\"l\" + \"r\" * (len(matr.columns) - 1)  \n",
    "                ))\n",
    "    else:\n",
    "        matr = all_scenario_matrices(results_df, group_cols=group_cols)\n",
    "        for name, matrix in matr.items():\n",
    "            matrix = matrix.reset_index()\n",
    "\n",
    "            save_pdf_tex = folder / f\"corr_matrix_{name}.tex\"\n",
    "            save_pdf_csv = folder / f\"corr_matrix_{name}.csv\"\n",
    "            #saving csv file\n",
    "            matrix.to_csv(save_pdf_csv, index=False, encoding=\"utf-8\", sep=\";\")\n",
    "            #saving latex file\n",
    "            with open(save_pdf_tex, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(matrix.to_latex(\n",
    "                    index=False,\n",
    "                    escape=True,\n",
    "                    float_format=\"%.0f\",  \n",
    "                    caption=f\"Correlation Matrix FV vs EDA - {name}\",\n",
    "                    label=f\"tab:Corr_Matrix_FV_EDA_{name}\",\n",
    "                    column_format=\"l\" + \"r\" * (len(matrix.columns) - 1)  \n",
    "                ))\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e8979",
   "metadata": {},
   "source": [
    "### Execution Method for the correlation test\n",
    "\n",
    "Grouping the data into multiple Groups\n",
    "1. General (All Data is combined together)\n",
    "2. Decision Phases: For each decision category applying the correlation matrix\n",
    "3. Session\n",
    "4. Transaction types: For each transaction category applying the correlation matrix\n",
    "5. Session √ó Decision Phase\n",
    "6. Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75965baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eda_signals = [\"scr_amplitude\", \"scr_auc\", \"scr_count\", \"scr_per_sec\", \"scl_mean\", \"scl_std\"]\n",
    "\n",
    "market_mechanism = [\"rd_GUISTY_per_round\", \"rad_GUISTY_per_round\", \"rd_Kirchler_per_round\", \"rad_Kirchler_per_round\",\"rad_Smith_per_round\", \"rd_Smith_per_round\", \"V_t\", \"AMAD_t\", \"MAD_t\"]\n",
    "\n",
    "MATRIX_CORR = TESTS_RESULT / \"MATRIX_CORR\"\n",
    "os.makedirs(MATRIX_CORR, exist_ok=True)\n",
    "\n",
    "# A) All data combined\n",
    "results_all = compute_correlations(df_all, market_mechanism, groupby_cols=None)\n",
    "strong_or_weak_general, only_significant_general, not_significant_general, frequency_all = filter_results(results_all, None)\n",
    "frequency_all[\"Scenario\"] = \"General\"\n",
    "calc_and_save_matrices(results_all, group_cols=\"General\",folder = MATRIX_CORR)\n",
    "\n",
    "\n",
    "#print(table_all)\n",
    "\n",
    "\n",
    "#print(sign_bigger_r_all)\n",
    "\n",
    "# B) Grouped only by Decision Phase\n",
    "results_by_phase = compute_correlations(df_all,market_mechanism,  groupby_cols=[\"window_type\"])\n",
    "strong_or_weak_phase, only_significant_phase, not_significant_phase, frequency_phase = filter_results(results_by_phase, groupby_cols=[\"window_type\"])\n",
    "frequency_phase[\"Scenario\"] = \"Decision-Phase\"\n",
    "calc_and_save_matrices(results_by_phase, group_cols=[\"window_type\"],folder = MATRIX_CORR)\n",
    "#print(sign_bigger_r_phase)\n",
    "\n",
    "df_frequency = pd.concat([frequency_all ,frequency_phase ])\n",
    "\n",
    "# C) Grouped only by Session\n",
    "results_by_session = compute_correlations(df_all, market_mechanism, groupby_cols=[\"session\"])\n",
    "strong_or_weak_session, only_significant_session, not_significant_session, frequency_session  = filter_results(results_by_session,groupby_cols=[\"session\"])\n",
    "frequency_session[\"Scenario\"] = \"Session\"\n",
    "df_frequency = pd.concat([df_frequency,frequency_session ])\n",
    "calc_and_save_matrices(results_by_session, group_cols=[\"session\"],folder = MATRIX_CORR)\n",
    "#print(sign_bigger_r_session)\n",
    "\n",
    "# D) Grouped only by Transaction \n",
    "results_by_transaction = compute_correlations(df_all,market_mechanism, groupby_cols=[\"transaction\"])\n",
    "strong_or_weak_transaction, only_significant_transaction, not_significant_transaction, frequency_transaction = filter_results(results_by_transaction, groupby_cols=[\"transaction\"])\n",
    "frequency_transaction[\"Scenario\"] = \"Transaction\"\n",
    "#print(sign_bigger_r_transaction)\n",
    "df_frequency = pd.concat([df_frequency,frequency_transaction ])\n",
    "calc_and_save_matrices(results_by_transaction, group_cols=[\"transaction\"],folder = MATRIX_CORR)\n",
    "\n",
    "# D) Grouped by Session √ó Decision Phase\n",
    "results_by_session_phase = compute_correlations(df_all, market_mechanism, groupby_cols=[\"session\", \"window_type\"])\n",
    "strong_or_weak_session_phase , only_significant_phase, not_significant_session_phase, frequency_session_phase  = filter_results(results_by_session_phase, groupby_cols=[\"session\", \"window_type\"])\n",
    "df_frequency = pd.concat([df_frequency,frequency_session_phase ])\n",
    "#print(sign_bigger_r_session_phase)\n",
    "\n",
    "\n",
    "#D) Peformance\n",
    "print(\"\\nperformance\")\n",
    "performance = [\"final_money\"]\n",
    "results_performance = compute_correlations(df_all, performance, groupby_cols=None)\n",
    "strong_or_weak_perfomance,  only_significant_perfomance, not_significant_performance, frequency_performance  = filter_results(results_performance, None)\n",
    "frequency_performance[\"Scenario\"] = \"Performance\"\n",
    "df_frequency = pd.concat([df_frequency,frequency_performance ])\n",
    "calc_and_save_matrices(results_performance, group_cols=\"Performance\",folder = MATRIX_CORR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_frequency, df_frequency_rel = combine_frequency(df_frequency)\n",
    "\n",
    "#Saving df_frequency and df_frequency_rel in latex\n",
    "\n",
    "abs_path = TESTS_RESULT / \"frequency_absolute_corr.tex\"\n",
    "rel_path = TESTS_RESULT / \"frequency_relative_corr.tex\"\n",
    "abs_path_csv = TESTS_RESULT / \"frequency_absolute_corr.csv\"\n",
    "rel_path_csv = TESTS_RESULT / \"frequency_relative_corr.csv\"\n",
    "df_frequency.to_csv(abs_path_csv, index=False, encoding=\"utf-8\", sep=\";\")\n",
    "df_frequency_rel.to_csv(rel_path_csv, index=False, encoding=\"utf-8\", sep=\";\")\n",
    "\n",
    "\n",
    "\n",
    "with open(abs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(df_frequency.to_latex(\n",
    "        index=False,\n",
    "        escape=True,\n",
    "        float_format=\"%.0f\",  \n",
    "        caption=\"Frequency of each Scenario with Correlationcategories\",\n",
    "        label=\"tab:frequency_absolute\",\n",
    "        column_format=\"l\" + \"r\" * (len(df_frequency.columns) - 1)  # links + rechtsb√ºndig\n",
    "    ))\n",
    "\n",
    "with open(rel_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(df_frequency_rel.to_latex(\n",
    "        index=False,\n",
    "        escape=True,\n",
    "        float_format=\"%.1f\", \n",
    "        caption=\"Relative frequency (%) of each Scenario with Correlationcategories\",\n",
    "        label=\"tab:frequency_relative\",\n",
    "        column_format=\"l\" + \"r\" * (len(df_frequency_rel.columns) - 1)\n",
    "    ))\n",
    "\n",
    "print(f\"‚úÖ Exported Tables:\\n- {abs_path}\\n- {rel_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49867e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_or_weak_session_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "market_mechanism = [\"V_t\", \"MAD_t\", \"AMAD_t\"]\n",
    "\n",
    "# A) All data combined\n",
    "results_all_Vol = compute_correlations(df_all, market_mechanism, groupby_cols=None)\n",
    "sign_bigger_r_all_Vol, significant_all_Vol, not_significant_all_Vol, frequency_all_Vol = filter_results(results_all_Vol, None)\n",
    "frequency_all_Vol[\"Scenario\"] = \"General\"\n",
    "\n",
    "\n",
    "\n",
    "#print(sign_bigger_r_all)\n",
    "# B) Grouped only by Decision Phase\n",
    "results_by_phase_Vol = compute_correlations(df_all,market_mechanism,  groupby_cols=[\"window_type\"])\n",
    "sign_bigger_r_phase_Vol, significant_phase_Vol, not_significant_phase_Vol, frequency_phase_VOl= filter_results(results_by_phase_Vol, groupby_cols=[\"window_type\"])\n",
    "frequency_phase_VOl[\"Scenario\"] = \"Decision-Phase\"\n",
    "\n",
    "df_frequency_VOl = pd.concat([frequency_all_Vol ,frequency_phase_VOl ])\n",
    "#print(sign_bigger_r_phase)\n",
    "\n",
    "# C) Grouped only by Session\n",
    "results_by_session_Vol = compute_correlations(df_all, market_mechanism,  groupby_cols=[\"session\"])\n",
    "sign_bigger_r_session_Vol, significant_session_Vol, not_significant_session_Vol, frequency_session_VOl = filter_results(results_by_session_Vol,groupby_cols=[\"session\"])\n",
    "frequency_session_VOl[\"Scenario\"] = \"Session\"\n",
    "df_frequency_VOl = pd.concat([df_frequency_VOl ,frequency_session_VOl ])\n",
    "#print(sign_bigger_r_session)\n",
    "# C) Grouped only by Transaction \n",
    "results_by_transaction_Vol = compute_correlations(df_all,market_mechanism, groupby_cols=[\"transaction\"])\n",
    "sign_bigger_r_transaction_Vol, significant_transaction_Vol, not_significant_transaction_Vol, frequency_transaction_VOl = filter_results(results_by_transaction_Vol, groupby_cols=[\"transaction\"])\n",
    "frequency_transaction_VOl[\"Scenario\"] = \"Transaction\"\n",
    "df_frequency_VOl = pd.concat([df_frequency_VOl ,frequency_transaction_VOl ])\n",
    "\n",
    "\n",
    "# D) Grouped by Session √ó Decision Phase\n",
    "results_by_session_phase_Vol = compute_correlations(df_all, market_mechanism, groupby_cols=[\"session\", \"window_type\"])\n",
    "sign_bigger_r_session_phase_Vol , significant_session_phase_Vol, not_significant_session_phase_Vol, frequency_session_phase_VOl  = filter_results(results_by_session_phase_Vol, groupby_cols=[\"session\", \"window_type\"])\n",
    "frequency_session_phase_VOl[\"Scenario\"] = \"Session and Decision-Phase\"\n",
    "df_frequency_VOl = pd.concat([df_frequency_VOl ,frequency_session_phase_VOl ])\n",
    "#print(sign_bigger_r_session_phase)\n",
    "\n",
    "#D) Peformance\n",
    "performance = [\"final_money\"]\n",
    "print(\"\\nperformance\")\n",
    "results_performance_Vol = compute_correlations(df_all, performance, groupby_cols=None)\n",
    "sign_bigger_r_perfomance_Vol , significant_performance_Vol, not_significant_performance_Vol, frequency_performance_VOl  = filter_results(results_performance_Vol, None)\n",
    "frequency_performance_VOl[\"Scenario\"] = \"Performance\"\n",
    "df_frequency_VOl = pd.concat([df_frequency_VOl ,frequency_performance_VOl ])\n",
    "\n",
    "\n",
    "df_frequency_vol, df_frequency_rel_vol = combine_frequency(df_frequency_VOl)\n",
    "\n",
    "#Saving df_frequency and df_frequency_rel in latex\n",
    "\n",
    "abs_path_vol = TESTS_RESULT / \"frequency_absolute_corr_VOl.tex\"\n",
    "rel_path_vol = TESTS_RESULT / \"frequency_relative_corr_VOl.tex\"\n",
    "\n",
    "with open(abs_path_vol, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(df_frequency_vol.to_latex(\n",
    "        index=False,\n",
    "        escape=True,\n",
    "        float_format=\"%.0f\",  \n",
    "        caption=\"Frequency of each Scenario with Correlationcategories\",\n",
    "        label=\"tab:frequency_absolute\",\n",
    "        column_format=\"l\" + \"r\" * (len(df_frequency_vol.columns) - 1)  # links + rechtsb√ºndig\n",
    "    ))\n",
    "\n",
    "with open(rel_path_vol, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(df_frequency_vol.to_latex(\n",
    "        index=False,\n",
    "        escape=True,\n",
    "        float_format=\"%.1f\", \n",
    "        caption=\"Relative frequency (%) of each Scenario with Correlationcategories\",\n",
    "        label=\"tab:frequency_relative\",\n",
    "        column_format=\"l\" + \"r\" * (len(df_frequency_rel_vol.columns) - 1)\n",
    "    ))\n",
    "\n",
    "print(f\"‚úÖ Exported Tables:\\n- {abs_path}\\n- {rel_path}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e4652",
   "metadata": {},
   "source": [
    "## Mann-Whitney-U test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41347bd1",
   "metadata": {},
   "source": [
    "Whitney U-Test is a non-paramteric version of the 2 paired t-test\n",
    "- Men vs Women\n",
    "- Risk and Non risky participants\n",
    "- Risk and Non-risky sessions\n",
    "- High RD <> Low RD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ae1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_all.copy()\n",
    "test[\"performance_classification\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe56ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whitney = df_all.copy()\n",
    "\n",
    "def cal_whitneyU_test(df_whitney, y_value, group_column, combinations_group, side, ses_or_part):\n",
    "    \"\"\"\n",
    "    Method for applying Mann-Whitney-U test.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    #retrive the group categories\n",
    "    for groupA, groupB in combinations_group:\n",
    "        #split the categories into 2 groups + calculating the median of the EDA signal (of each participant or session)\n",
    "        group_A = df_whitney[df_whitney[group_column] == groupA]\n",
    "        a_mean = (group_A.groupby([ses_or_part])[y_value].median().reset_index(name = f\"mean_{y_value}\"))\n",
    "        group_B = df_whitney[df_whitney[group_column] == groupB]\n",
    "        b_mean = (group_B.groupby([ses_or_part])[y_value].median().reset_index(name = f\"mean_{y_value}\"))\n",
    "\n",
    "        #check valid length of sessions\n",
    "        n1,n2 =  len(a_mean), len(b_mean)\n",
    "        if n1 ==0 or n2 == 0:\n",
    "            continue\n",
    "        \n",
    "        #Test\n",
    "        statistic, p_value = stats.mannwhitneyu(a_mean[f\"mean_{y_value}\"], b_mean[f\"mean_{y_value}\"], alternative=side, method = 'auto')\n",
    "\n",
    "        #calcualting meanU, p-correction\n",
    "        mean_U = n1 * n2 / 2\n",
    "        var_U = n1 * n2 * (n1 + n2 + 1) / 12\n",
    "        std_U = var_U ** 0.5\n",
    "        z = (statistic - mean_U) / std_U  \n",
    "        p_value_z = 2 * stats.norm.sf(abs(z)) \n",
    "\n",
    "        print(f\"\\n {groupA} vs. {groupB}\")\n",
    "        print(f\"Mann-Whitney U statistic: {statistic}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "        print(f\"p-Correction: {p_value_z}\")\n",
    " \n",
    "        meaning = None\n",
    "        #calcualte effect size r \n",
    "        N = n1 + n2\n",
    "        #print(N)\n",
    "        r = (abs(z) / math.sqrt(N))\n",
    "        print(f\"Effect size r: {r:.3f}\")\n",
    "\n",
    "        #Check if p-value < 0.05 and if effect size is bigger than 0.3\n",
    "\n",
    "        if np.isnan(p_value):\n",
    "            meaning = \"p_value is None\"\n",
    "        elif p_value > 0.05:\n",
    "            meaning = \"not significant\"\n",
    "        elif  r >= 0.5 :\n",
    "            meaning = f\"strong effect\"\n",
    "        elif r >= 0.3 and r < 0.5:\n",
    "                meaning = f\"medium effect\"\n",
    "        else: meaning = f\"weak effect difference\"\n",
    "        print(meaning)\n",
    "\n",
    "        results.append({\"sess_or_part\": ses_or_part, \"Group_label\": group_column, \"y_value\": y_value, \"combination\": f\"{groupA} vs. {groupB}\", \"statistic\": statistic, \"p_value\": p_value, \"p_value_z\": p_value_z, \"z\":z, \"n1\":n1, \"n2\":n2, \"effect_size_r\": r,  \"meaning\": meaning})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def cell_format(r, p_col =\"p_value\", val=\"statistic\", show_r=True):\n",
    "    \"\"\"\n",
    "    Method for determine the cell content of the matrix, which consists of the effect-size and the significant marks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base = f\"{float(r[val]):.3f}\"\n",
    "    except Exception:\n",
    "        base = str(r[val])\n",
    "    if p_col in r and pd.notna(r[p_col]):\n",
    "        base += significance_marks(r[p_col])\n",
    "    if show_r and val != \"effect_size_r\"  and \"effect_size_r\" in r and pd.notna(r[\"effect_size_r\"]):\n",
    "        base += f\"; r={r['effect_size_r']:.2f}\"\n",
    "    return base\n",
    "\n",
    "\n",
    "def make_man_Whithney_U_matrix(df_manU, group_label = None, value_col= \"statistic\", add_stars= True, only_significant= False ):\n",
    "    \"\"\"\n",
    "        Method for creating a matrix based on the results of the test\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df_manU.copy()\n",
    "\n",
    "    if group_label is not None:\n",
    "        df = df[df[\"Group_label\"] == group_label]\n",
    "\n",
    "    #check significance\n",
    "    if only_significant:\n",
    "        sig_combos = (\n",
    "            df[df[\"p_value\"] <= 0.05]\n",
    "            .groupby([\"Group_label\", \"sess_or_part\", \"combination\"])\n",
    "            .size()\n",
    "            .reset_index()[[\"Group_label\", \"sess_or_part\", \"combination\"]]\n",
    "        )\n",
    "        if sig_combos.empty:\n",
    "            return pd.DataFrame([])\n",
    "\n",
    "        df = df.merge(sig_combos,\n",
    "                      on=[\"Group_label\", \"sess_or_part\", \"combination\"],\n",
    "                      how=\"inner\")\n",
    "\n",
    "    #applying the cell format method \n",
    "    df[\"__cell__\"] = df.apply(\n",
    "        lambda r: cell_format(r, p_col=\"p_value\", val=value_col, show_r=True) if add_stars\n",
    "                  else cell_format(r, p_col=None, val=value_col, show_r=True),\n",
    "        axis=1\n",
    "    )\n",
    "    #pivot the table to create a matrix format\n",
    "    matrix = df.pivot_table(\n",
    "                index = [\"Group_label\",\"sess_or_part\",\"combination\"], \n",
    "                columns = \"y_value\",\n",
    "                values= \"__cell__\",\n",
    "                aggfunc = \"first\",\n",
    "                fill_value=\"-\"\n",
    "                )\n",
    " \n",
    "    matrix = matrix.reindex(sorted(matrix.columns),axis=1)\n",
    "    matrix = matrix.reset_index()\n",
    "\n",
    "    \n",
    "\n",
    "        # optional an erste Stelle setzen:\n",
    "    first = [\"Group_label\"] + [c for c in matrix.columns if c != \"Group_label\"]\n",
    "    matrix = matrix[first]\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def saving_matrix_csv_latex(matrix, file_name, caption, label, FOLDER = TESTS_RESULT):\n",
    "    \"\"\"\n",
    "    Saving matrix as a .csv and latex file.\n",
    "    \"\"\"\n",
    "\n",
    "    if matrix is None:\n",
    "        raise ValueError(\"No matrix file\")\n",
    "\n",
    "    tex_path = FOLDER / f\"{file_name}.tex\"\n",
    "    csv_path = FOLDER / f\"{file_name}.csv\"\n",
    "    matrix.to_csv(csv_path, index=False, encoding=\"utf-8\", sep=\";\")\n",
    "\n",
    "    with open(tex_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(matrix.to_latex(\n",
    "            index=False,\n",
    "            escape=True,\n",
    "            float_format=\"%.03f\",  \n",
    "            caption=f\"{caption}\",\n",
    "            label=f\"{label}\",\n",
    "            column_format=\"l\" + \"r\" * (len(matrix.columns) - 1)  \n",
    "        ))\n",
    "\n",
    "\n",
    "    print(f\"‚úÖ Exported Tables:\\n- {tex_path}\\n- {csv_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"======================= two-sided ========================\")\n",
    "all_res = []\n",
    "\n",
    "#columns for participant comparision\n",
    "group_participant = [\"gender\", \"performance_classification\", \"performance_classification_session_wise\", \"p_Risk_Guisty_categorization_average\",\"p_Risk_Smith_categorization_average\", \"p_Risk_Kirchler_categorization_average\", \"Vol_Classification_Global\", \"rd_Smith_categorization\", \"Category_Guisty_round\", \"rd_Kirchler_categorization\"]\n",
    "# columns for session comparisons\n",
    "group_session = [\"performance_classification_session\", \"s_Risk_Guisty_categorization_average\",\"s_Risk_Smith_categorization_average\", \"s_Risk_Kirchler_categorization_average\", \"Vol_Classification_between_session\", \"Category_SMITH\", \"Category_Guisty\", \"Category_KIRCHLER\" ]\n",
    "\n",
    "#EDA signals which are used for the statistical tests\n",
    "eda_sign = [\"scr_auc\", \"scr_per_sec\", \"scl_mean\", \"scl_std\"]\n",
    "sess_or_part = [\"participant\", \"session\"]\n",
    "\n",
    "#applying the mannwhitney U test\n",
    "for group in group_participant:\n",
    "    print(\"\\n\",group)\n",
    "    for eda in eda_sign: \n",
    "        print(\"\\n\",eda)\n",
    "        group_values = df_whitney[group].dropna().unique()      #retrieve unique values of group-columns\n",
    "        #creating pariwise combiantions\n",
    "        combinations_ = list(list(itertools.combinations(group_values, 2)))\n",
    "        print(combinations_)\n",
    "        res_part = cal_whitneyU_test(df_whitney, eda, group, combinations_, 'two-sided', \"participant\")\n",
    "        all_res.append(res_part)\n",
    "\n",
    "#session-wise comaprison\n",
    "for group in group_session:\n",
    "    print(\"\\n\",group)\n",
    "    for eda in eda_sign: \n",
    "        print(\"\\n\",eda)\n",
    "        group_values = df_whitney[group].dropna().unique()\n",
    "        combinations_ = list(list(itertools.combinations(group_values, 2)))\n",
    "        print(combinations_)\n",
    "        res_sess = cal_whitneyU_test(df_whitney, eda, group, combinations_, 'two-sided', \"session\")\n",
    "        all_res.append(res_sess)\n",
    "#concatenate each results\n",
    "df_man_whitneyU = pd.concat(all_res, ignore_index=True)\n",
    "\n",
    "#transoform results into a matrix dataframe and save them as a latex and csv file\n",
    "matrirx_manU = make_man_Whithney_U_matrix(df_man_whitneyU, only_significant=True)\n",
    "matrirx_manU_all = make_man_Whithney_U_matrix(df_man_whitneyU, only_significant=None)\n",
    "saving_matrix_csv_latex(matrirx_manU, file_name =\"MannWhitneyU_two_sided\", caption=\"Mann Whitney U test - two sided\", label =\"tab:ManWhitneyU_test_2sided\" )\n",
    "saving_matrix_csv_latex(matrirx_manU_all, file_name =\"MannWhitneyU_two_sided_all_values\", caption=\"Mann Whitney U test - two sided - All data\", label =\"tab:ManWhitneyU_test_2sided_all_data\" )\n",
    "matrirx_manU\n",
    "\n",
    "#separating the full results into SCR and SCL tables\n",
    "base_cols = [\"Group_label\",\"combination\"]\n",
    "scl_cols = [c for c in [\"scl_mean\", \"scl_std\"] if c  in matrirx_manU_all.columns ]\n",
    "matrix_scl = matrirx_manU_all[base_cols + scl_cols].copy()\n",
    "saving_matrix_csv_latex(matrix_scl, file_name =\"MannWhitneyU_two_sided_SCL\", caption=\"Mann Whitney U test - two sided - SCL all\", label =\"tab:ManWhitneyU_test_2sided_SCL\" )\n",
    "scr_cols = [c for c in [\"scr_auc\", \"scr_per_sec\"] if c  in matrirx_manU_all.columns ]\n",
    "matrix_scr = matrirx_manU_all[base_cols + scr_cols].copy()\n",
    "saving_matrix_csv_latex(matrix_scr, file_name =\"MannWhitneyU_two_sided_SCR\", caption=\"Mann Whitney U test - two sided - SCR all\", label =\"tab:ManWhitneyU_test_2sided_SCR\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrirx_manU_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67761613",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_man_whitneyU[df_man_whitneyU[\"p_value\"] < 0.05 ]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae4abc",
   "metadata": {},
   "source": [
    "### Man-Whitney-U test - greater\n",
    "\n",
    "Applying one sided ManWhitneyU test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"======================= greater ========================\")\n",
    "#participant wise\n",
    "group_participant = [\"gender\", \"performance_classification\", \"performance_classification_session_wise\", \"p_Risk_Guisty_categorization_average\",\"p_Risk_Smith_categorization_average\", \"p_Risk_Kirchler_categorization_average\", \"Vol_Classification_Global\", \"rd_Smith_categorization\", \"Category_Guisty_round\", \"rd_Kirchler_categorization\"]\n",
    "#session-wise\n",
    "group_session = [\"performance_classification_session\", \"s_Risk_Guisty_categorization_average\",\"s_Risk_Smith_categorization_average\", \"s_Risk_Kirchler_categorization_average\", \"Vol_Classification_between_session\", \"Category_SMITH\", \"Category_Guisty\", \"Category_KIRCHLER\" ]\n",
    "\n",
    "eda_sign = [\"scr_auc\", \"scr_per_sec\", \"scl_mean\", \"scl_std\"]\n",
    "sess_or_part = [\"participant\", \"session\"]\n",
    "\n",
    "#participant\n",
    "all_res_greater = []\n",
    "for group in group_participant:\n",
    "    print(\"\\n\",group)\n",
    "    for eda in eda_sign: \n",
    "        print(\"\\n\",eda)\n",
    "        group_values = df_whitney[group].dropna().unique()\n",
    "        combinations_ = list(list(itertools.combinations(group_values, 2)))\n",
    "        print(combinations_)\n",
    "        res_part_great = cal_whitneyU_test(df_whitney, eda, group, combinations_, 'greater', \"participant\")\n",
    "        all_res_greater.append(res_part_great)\n",
    "#session-wise\n",
    "for group in group_session:\n",
    "    print(\"\\n\",group)\n",
    "    for eda in eda_sign: \n",
    "        print(\"\\n\",eda)\n",
    "        group_values = df_whitney[group].dropna().unique()\n",
    "        combinations_ = list(list(itertools.combinations(group_values, 2)))\n",
    "        print(combinations_)\n",
    "        res_sess_great = cal_whitneyU_test(df_whitney, eda, group, combinations_, 'greater', \"session\")\n",
    "        all_res_greater.append(res_sess_great)\n",
    "\n",
    "df_man_whitneyU_greater = pd.concat(all_res_greater, ignore_index=True)\n",
    "\n",
    "#creating matrix\n",
    "matrix_manU_greater = make_man_Whithney_U_matrix(df_man_whitneyU_greater, only_significant=True)\n",
    "saving_matrix_csv_latex(matrix_manU_greater, file_name =\"MannWhitneyU_greater\", caption=\"Mann Whitney U test - greater\", label =\"tab:ManWhitneyU_test_greater\" )\n",
    "matrix_manU_greater\n",
    "\n",
    "#separating the full results into 2 table groups\n",
    "base_cols = [\"Group_label\",\"combination\"]\n",
    "scl_cols = [c for c in [\"scl_mean\", \"scl_std\"] if c  in matrirx_manU_all.columns ]\n",
    "matrix_scl_gr = matrix_manU_greater[base_cols + scl_cols].copy()\n",
    "saving_matrix_csv_latex(matrix_scl_gr, file_name =\"MannWhitneyU_greater_SCL\", caption=\"Mann Whitney U test - greater - SCL all\", label =\"tab:ManWhitneyU_test_greater_SCL\" )\n",
    "scr_cols = [c for c in [\"scr_auc\", \"scr_per_sec\"] if c  in matrirx_manU_all.columns ]\n",
    "matrix_scr_gr = matrix_manU_greater[base_cols + scr_cols].copy()\n",
    "saving_matrix_csv_latex(matrix_scr_gr, file_name =\"MannWhitneyU_greater_SCR\", caption=\"Mann Whitney U test -greater - SCR all\", label =\"tab:ManWhitneyU_test_greater_SCR\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0211f",
   "metadata": {},
   "source": [
    "### Man-Whitney-U test - less\n",
    "\n",
    "Applying one sided ManWhitneyU test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======================= less ========================\")\n",
    "group_participant = [\"gender\", \"performance_classification\", \"performance_classification_session_wise\", \"p_Risk_Guisty_categorization_average\",\"p_Risk_Smith_categorization_average\", \"p_Risk_Kirchler_categorization_average\", \"Vol_Classification_Global\"]\n",
    "group_session = [\"performance_classification_session\", \"s_Risk_Guisty_categorization_average\",\"s_Risk_Smith_categorization_average\", \"s_Risk_Kirchler_categorization_average\", \"Vol_Classification_between_session\"]\n",
    "\n",
    "eda_sign = [\"scr_auc\", \"scr_per_sec\", \"scl_mean\", \"scl_std\"]\n",
    "sess_or_part = [\"participant\", \"session\"]\n",
    "\n",
    "#participated\n",
    "all_res_less = []\n",
    "for group in group_participant:\n",
    "    print(\"\\n\",group)\n",
    "    for eda in eda_sign: \n",
    "        print(\"\\n\",eda)\n",
    "        group_values = df_whitney[group].unique()\n",
    "        combinations_ = list(list(itertools.combinations(group_values, 2)))\n",
    "        print(combinations_)\n",
    "        res_part_less = cal_whitneyU_test(df_whitney, eda, group, combinations_, 'less', \"participant\")     #direction : A less B\n",
    "        all_res_less.append(res_part_less)\n",
    "\n",
    "#session-wise\n",
    "for group in group_session:\n",
    "    print(\"\\n\",group)\n",
    "    for eda in eda_sign: \n",
    "        print(\"\\n\",eda)\n",
    "        group_values = df_whitney[group].unique()\n",
    "        combinations_ = list(list(itertools.combinations(group_values, 2)))\n",
    "        print(combinations_)\n",
    "        res_sess_less = cal_whitneyU_test(df_whitney, eda, group, combinations_, 'less', \"session\")        #direction : A less B\n",
    "        all_res_less.append(res_sess_less)\n",
    "\n",
    "df_man_whitneyU_less = pd.concat(all_res_less, ignore_index=True)\n",
    "\n",
    "#creating matrix\n",
    "matrix_manU_less = make_man_Whithney_U_matrix(df_man_whitneyU_less, only_significant=True)\n",
    "saving_matrix_csv_latex(matrix_manU_greater, file_name =\"MannWhitneyU_less\", caption=\"Mann Whitney U test - less\", label =\"tab:ManWhitneyU_test_less\" )\n",
    "matrix_manU_less\n",
    "\n",
    "#separating into 2 tables \n",
    "base_cols = [\"Group_label\",\"combination\"]\n",
    "scl_cols = [c for c in [\"scl_mean\", \"scl_std\"] if c  in matrirx_manU_all.columns ]\n",
    "matrix_scl_less = matrix_manU_less[base_cols + scl_cols].copy()\n",
    "saving_matrix_csv_latex(matrix_scl_less, file_name =\"MannWhitneyU_less_SCL\", caption=\"Mann Whitney U test - less - SCL all\", label =\"tab:ManWhitneyU_test_less_SCL\" )\n",
    "scr_cols = [c for c in [\"scr_auc\", \"scr_per_sec\"] if c  in matrirx_manU_all.columns ]\n",
    "matrix_scr_less = matrix_manU_less[base_cols + scr_cols].copy()\n",
    "saving_matrix_csv_latex(matrix_scr_less, file_name =\"MannWhitneyU_less_SCR\", caption=\"Mann Whitney U test - less - SCR all\", label =\"tab:ManWhitneyU_test_less_SCR\" )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66fcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_man_whitneyU_less"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481fb4a",
   "metadata": {},
   "source": [
    "## Wilcoxon \n",
    "\n",
    "After applying the nonparametric independent test, now the dependent Wilcoxon test will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wilcoxon = df_all.copy()\n",
    "\n",
    "def cal_wilcoxon(df_wilcoxon, y_value, group_column, combinations_group, side, sess_or_part):\n",
    "    \"\"\"\n",
    "        Main test method for the wilcoxon test.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for groupA, groupB in combinations_group:\n",
    "        #seperating into 2 groups, Calculating the median based on \"participant\" and \"session\"\n",
    "        group_A = df_wilcoxon[df_wilcoxon[group_column] == groupA]\n",
    "        #group by \"session_or_part\" and calculate the mean\n",
    "        a_mean = (group_A.groupby([sess_or_part])[y_value].median().reset_index(name = f\"mean_{y_value}_A\"))\n",
    "        group_B = df_wilcoxon[df_wilcoxon[group_column] == groupB]\n",
    "        b_mean = (group_B.groupby([sess_or_part])[y_value].median().reset_index(name = f\"mean_{y_value}_B\"))\n",
    "\n",
    "        #find pairs by merging the 2 groups\n",
    "        merged = pd.merge(a_mean, b_mean, on=sess_or_part, how='inner')\n",
    "\n",
    "        x = merged[f\"mean_{y_value}_A\"]\n",
    "        y = merged[f\"mean_{y_value}_B\"]\n",
    "        print(f\"{groupA} vs. {groupB}: n_pairs = {len(merged)}\")\n",
    "        #calc statistiscs\n",
    "        statistic, p_value = stats.wilcoxon(x, y,  alternative=side, method = 'auto')\n",
    "    \n",
    "        print(f\"\\n {groupA} vs. {groupB}\")\n",
    "        print(f\"Wilcoxon statistic: {statistic}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "        N = len(merged)\n",
    "        mean_W = N * (N + 1) / 4\n",
    "        var_W = N * (N + 1) * (2 * N + 1) / 24\n",
    "        std_W = math.sqrt(var_W)\n",
    "\n",
    "        z = (statistic - mean_W) / std_W\n",
    "        r = abs(z) / math.sqrt(N)\n",
    "        print(f\"z: {z:.3f}, r: {r:.3f}\")\n",
    "\n",
    "\n",
    "        #calcualte effect size r \n",
    "        N = len(a_mean) + len(b_mean)\n",
    "        print(N)\n",
    "        r = (abs(z) / math.sqrt(N))\n",
    "        print(f\"Effect size r: {r:.3f}\")\n",
    "\n",
    "        if np.isnan(p_value):\n",
    "            meaning = \"p_value is None\"\n",
    "        elif p_value > 0.05:\n",
    "            meaning = \"not significant\"\n",
    "        elif r >= 0.5:\n",
    "            meaning = \"strong effect\"\n",
    "        elif r >= 0.3:\n",
    "            meaning = \"medium effect\"\n",
    "        else:\n",
    "            meaning = \"weak effect difference\"\n",
    "\n",
    "        results.append({\"sess_or_part\": sess_or_part, \"Group_label\": group_column, \"y_value\": y_value, \"combination\": f\"{groupA} vs. {groupB}\", \"statistic\": statistic, \"p_value\": p_value,\"effect_size_r\": r,  \"meaning\": meaning})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "def make_Wilcoxon_matrix(df_manU, group_label = None, value_col= \"statistic\", add_stars= True, only_significant= False ):\n",
    "    \"\"\"\n",
    "        Creating Wilcoxon matrix for easier visulaization.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df_manU.copy()\n",
    "\n",
    "    if group_label is not None:\n",
    "        df = df[df[\"Group_label\"] == group_label]\n",
    "\n",
    "    if only_significant:\n",
    "        sig_combos = (\n",
    "            df[df[\"p_value\"] <= 0.05]\n",
    "            .groupby([\"Group_label\", \"sess_or_part\", \"combination\"])\n",
    "            .size()\n",
    "            .reset_index()[[\"Group_label\", \"sess_or_part\", \"combination\"]]\n",
    "        )\n",
    "        if sig_combos.empty:\n",
    "            return pd.DataFrame([])\n",
    "\n",
    "        df = df.merge(sig_combos,\n",
    "                      on=[\"Group_label\", \"sess_or_part\", \"combination\"],\n",
    "                      how=\"inner\")\n",
    "\n",
    "    df[\"__cell__\"] = df.apply(\n",
    "        lambda r: cell_format(r, p_col=\"p_value\", val=value_col, show_r=True) if add_stars\n",
    "                  else cell_format(r, p_col=None, val=value_col, show_r=True),\n",
    "        axis=1\n",
    "    )\n",
    "    matrix = df.pivot_table(\n",
    "                index = [\"Group_label\",\"sess_or_part\",\"combination\"], \n",
    "                columns = \"y_value\",\n",
    "                values= \"__cell__\",\n",
    "                aggfunc = \"first\",\n",
    "                fill_value=\"-\"\n",
    "                )\n",
    " \n",
    "    matrix = matrix.reindex(sorted(matrix.columns),axis=1)\n",
    "    matrix = matrix.reset_index()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    first = [ \"Group_label\"] + [c for c in matrix.columns if c != \"Group_label\"]\n",
    "    matrix = matrix[first]\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "\n",
    "all_res = []        #result list\n",
    "#list of colums (categories) that are suppossed to be tested\n",
    "session = [\"Risk_Guisty\", \"Risk_Kirchler\", \"Risk_Smith\", \"Vol_Classification_Global\", \"Vol_Classification_intra_session\", \"window_type\", \"transaction\"]\n",
    "#EDA variables\n",
    "eda_sign = [\"scr_amplitude\", \"scr_auc\", \"scr_per_sec\", \"scl_mean\", \"scl_std\"]\n",
    "#applying on participant level\n",
    "for group in session:\n",
    "    for eda in eda_sign: \n",
    "        group_values = df_wilcoxon[group].dropna().unique()\n",
    "        #create combinations\n",
    "        combinations_ = list(list(itertools.combinations(group_values, 2)))\n",
    "        print(f\"{eda} vs. {group}\")\n",
    "        res_part = cal_wilcoxon(df_wilcoxon, eda, group, combinations_,\"two-sided\", \"participant\")\n",
    "        all_res.append(res_part)\n",
    "#applying on session level\n",
    "for group in session:\n",
    "    for eda in eda_sign: \n",
    "        group_values = df_wilcoxon[group].dropna().unique()\n",
    "        #create combinations\n",
    "        combinations_ = list(list(itertools.combinations(group_values, 2)))\n",
    "        print(f\"{eda} vs. {group}\")\n",
    "        res_sess = cal_wilcoxon(df_wilcoxon, eda, group, combinations_,\"two-sided\", \"session\")\n",
    "        all_res.append(res_sess)\n",
    "\n",
    "df_wilcoxon_res = pd.concat(all_res, ignore_index=True)\n",
    "\n",
    "#creating matrix\n",
    "matrix_wilcoxon = make_Wilcoxon_matrix(df_wilcoxon_res, only_significant=True)\n",
    "#saving matrix\n",
    "saving_matrix_csv_latex(matrix_wilcoxon, file_name =\"Wilcoxon_two_sided\", caption=\"Wilcoxon test - two-sided\", label =\"tab:Wilcoxon_2_sided\" )\n",
    "\n",
    "matrix_wilcoxon\n",
    "\n",
    "base_cols = [\"Group_label\", \"sess_or_part\",\"combination\"]\n",
    "scl_cols = [c for c in [\"scl_mean\", \"scl_std\"] if c  in matrix_wilcoxon.columns ]\n",
    "matrix_scl_2s_wilc = matrix_wilcoxon[base_cols + scl_cols].copy()\n",
    "saving_matrix_csv_latex(matrix_scl_2s_wilc, file_name =\"Wilcoxon_2s_SCR_2s_SCL\", caption=\"Wilcoxon test - 2sided - SCL\", label =\"tab:Wilcoxon_test_2s_SCL\" )\n",
    "scr_cols = [c for c in [\"scr_auc\", \"scr_per_sec\"] if c  in matrix_wilcoxon.columns ]\n",
    "matrix_scr_2s_wilc = matrix_wilcoxon[base_cols + scr_cols].copy()\n",
    "saving_matrix_csv_latex(matrix_scr_2s_wilc, file_name =\"Wilcoxon_2s_SCR\", caption=\"Wilcoxon test - 2sided - SCR\", label =\"tab:Wilconxon_test_2s_SCR\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a80c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wilcoxon_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3bfcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_wilcoxon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb612bf8",
   "metadata": {},
   "source": [
    "### One sided Wilcoxon test - less\n",
    "\n",
    "Testing if Group A has a lower median EDA value than Group B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b2b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_less = []\n",
    "session = [\"Risk_Guisty\", \"Risk_Kirchler\", \"Risk_Smith\", \"Vol_Classification_Global\", \"Vol_Classification_intra_session\", \"window_type\", \"transaction\"]\n",
    "\n",
    "eda_sign = [\"scr_amplitude\", \"scr_auc\", \"scr_per_sec\", \"scl_mean\", \"scl_std\"]\n",
    "#testing on participant level\n",
    "for group in session:\n",
    "    for eda in eda_sign: \n",
    "        group_values = df_wilcoxon[group].dropna().unique()\n",
    "        #create combinations\n",
    "        combinations_ = list(list(itertools.combinations(group_values, 2)))\n",
    "        res_part = cal_wilcoxon(df_wilcoxon, eda, group, combinations_,\"less\", \"participant\")\n",
    "        all_res_less.append(res_part)\n",
    "#testing on session level\n",
    "for group in session:\n",
    "    for eda in eda_sign: \n",
    "        group_values = df_wilcoxon[group].dropna().unique()\n",
    "        combinations_ = list(list(itertools.combinations(group_values, 2)))\n",
    "        res_sess = cal_wilcoxon(df_wilcoxon, eda, group, combinations_,\"less\", \"session\")\n",
    "        all_res_less.append(res_sess)\n",
    "#concatenate the results to a lust\n",
    "df_wilcoxon_less = pd.concat(all_res_less, ignore_index=True)\n",
    "#create matriy and save it\n",
    "matrix_wilcoxon_less = make_Wilcoxon_matrix(df_wilcoxon_less, only_significant=True)\n",
    "saving_matrix_csv_latex(matrix_wilcoxon_less, file_name =\"Wilcoxon_less\", caption=\"Wilcoxon test - less\", label =\"tab:Wilcoxon_less\" )\n",
    "\n",
    "\n",
    "#Separting results into two groups (SCL and SCR), Saving it as a latex table\n",
    "base_cols = [\"Group_label\", \"sess_or_part\",\"combination\"]\n",
    "scl_cols = [c for c in [\"scl_mean\", \"scl_std\"] if c  in matrix_wilcoxon_less.columns ]\n",
    "matrix_scl_less_wilc = matrix_wilcoxon_less[base_cols + scl_cols].copy()\n",
    "saving_matrix_csv_latex(matrix_scl_less_wilc, file_name =\"Wilcoxon_less_SCL\", caption=\"Wilcoxon test - less - SCL\", label =\"tab:Wilcoxon_test_less_SCL\" )\n",
    "scr_cols = [c for c in [\"scr_auc\", \"scr_per_sec\"] if c  in matrix_wilcoxon_less.columns ]\n",
    "matrix_scr_less_wilc = matrix_wilcoxon_less[base_cols + scr_cols].copy()\n",
    "saving_matrix_csv_latex(matrix_scr_less_wilc, file_name =\"Wilcoxon_less_SCR\", caption=\"Wilcoxon test - less - SCR\", label =\"tab:Wilconxon_test_less_SCR\" )\n",
    "\n",
    "matrix_wilcoxon_less"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae023d4",
   "metadata": {},
   "source": [
    "### One sided Wilcoxon test - greater\n",
    "\n",
    "Testing if Group A has a higher median EDA value than Group B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_greater_wilc = []\n",
    "session = [\"Risk_Guisty\", \"Risk_Kirchler\", \"Risk_Smith\", \"Vol_Classification_Global\", \"Vol_Classification_intra_session\", \"window_type\", \"transaction\"]\n",
    "\n",
    "eda_sign = [\"scr_amplitude\", \"scr_auc\", \"scr_per_sec\", \"scl_mean\", \"scl_std\"]\n",
    "#participant level\n",
    "for group in session:\n",
    "    for eda in eda_sign: \n",
    "\n",
    "        group_values = df_wilcoxon[group].dropna().unique()\n",
    "        combinations_ = list(list(itertools.combinations(group_values, 2)))\n",
    "        res_part = cal_wilcoxon(df_wilcoxon, eda, group, combinations_,\"greater\", \"participant\")\n",
    "        all_res_greater_wilc.append(res_part)\n",
    "#session level\n",
    "for group in session:\n",
    "    for eda in eda_sign: \n",
    "        group_values = df_wilcoxon[group].dropna().unique()\n",
    "        combinations_ = list(list(itertools.combinations(group_values, 2)))\n",
    "        res_sess = cal_wilcoxon(df_wilcoxon, eda, group, combinations_,\"greater\", \"session\")\n",
    "        all_res_greater_wilc.append(res_sess)\n",
    "\n",
    "df_wilcoxon_greater = pd.concat(all_res_greater_wilc, ignore_index=True)\n",
    "\n",
    "matrix_wilcoxon_greater = make_Wilcoxon_matrix(df_wilcoxon_greater, only_significant=True)\n",
    "saving_matrix_csv_latex(matrix_wilcoxon_greater, file_name =\"Wilcoxon_greater\", caption=\"Wilcoxon test - greater\", label =\"tab:Wilcoxon_greater\" )\n",
    "\n",
    "\n",
    "#seperating and saving into two LaTex tables (SCR and SCL)\n",
    "base_cols = [\"Group_label\", \"sess_or_part\",\"combination\"]\n",
    "scl_cols = [c for c in [\"scl_mean\", \"scl_std\"] if c  in matrix_wilcoxon_greater.columns ]\n",
    "matrix_scl_greater_wilc = matrix_wilcoxon_greater[base_cols + scl_cols].copy()\n",
    "saving_matrix_csv_latex(matrix_scl_greater_wilc, file_name =\"Wilcoxon_greater_SCL\", caption=\"Wilcoxon test - greater - SCL\", label =\"tab:Wilcoxon_test_greater_SCL\" )\n",
    "#SCR\n",
    "scr_cols = [c for c in [\"scr_auc\", \"scr_per_sec\"] if c  in matrix_wilcoxon_greater.columns ]\n",
    "matrix_scr_greater_wilc = matrix_wilcoxon_greater[base_cols + scr_cols].copy()\n",
    "saving_matrix_csv_latex(matrix_scr_greater_wilc, file_name =\"Wilcoxon_greater_SCR\", caption=\"Wilcoxon test - greater - SCR\", label =\"tab:Wilconxon_test_greater_SCR\" )\n",
    "\n",
    "matrix_wilcoxon_greater"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8a5de",
   "metadata": {},
   "source": [
    "## Friedman test\n",
    "\n",
    "Applying the Friedman Test, which is a non-parametric test. Tests multiple groups together.\n",
    "\n",
    "Group_categories: \"Risk_Guisty\", \"Risk_Kirchler\", \"Risk_Smith\", \"Vol_Classification_Global\", \"Vol_Classification_intra_session\", \"window_type\", \"transaction\"\n",
    "\n",
    "eda_sign = [\"scr_amplitude\", \"scr_auc\", \"scr_per_sec\", \"scl_mean\", \"scl_std\"]\n",
    "\n",
    "Testing on participant and session level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_friedman = df_all.copy()\n",
    "\n",
    "def cal_friedman(df_friedman, y_value, Group_label, group_category, sess_or_part):\n",
    "    \"\"\"\n",
    "    Method for applying the Friedmanchisquare\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for group_combo in group_category:\n",
    "        group_values = []\n",
    "        min_len = None\n",
    "\n",
    "        for group in group_combo:\n",
    "            #Separating values based on group  and calculating the median EDA signal\n",
    "            group_df = df_friedman[df_friedman[Group_label] == group]\n",
    "            mean_group = (group_df.groupby([sess_or_part])[y_value].median().reset_index(name = f\"median_{y_value}\")).dropna()\n",
    "            # \n",
    "            group_values.append(mean_group[f\"median_{y_value}\"])\n",
    "            print(group_values)\n",
    "            if min_len is None or len(mean_group) < min_len:\n",
    "                min_len = len(mean_group)\n",
    "\n",
    "        #trimming for getting a groups of the same length\n",
    "        trimmed_arrays = [gv.iloc[:min_len].to_numpy(dtype=float) for gv in group_values]\n",
    "        #calling test method \n",
    "        statistic, p_value = stats.friedmanchisquare(*trimmed_arrays)\n",
    "    \n",
    "        print(f\"Mann-Whitney U statistic: {statistic}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "\n",
    "        results.append({\"sess_or_part\": sess_or_part, \"Group_label\": Group_label, \"y_value\": y_value,\"combination\": group_combo,  \"statistic\": statistic, \"p_value\": p_value })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "all_res_friedman = []\n",
    "session = [\"Risk_Guisty\", \"Risk_Kirchler\", \"Risk_Smith\", \"Vol_Classification_Global\", \"Vol_Classification_intra_session\", \"window_type\", \"transaction\"]\n",
    "\n",
    "eda_sign = [\"scr_amplitude\", \"scr_auc\", \"scr_per_sec\", \"scl_mean\", \"scl_std\"]\n",
    "for group in session:\n",
    "    for eda in eda_sign: \n",
    "        #find unique group values\n",
    "        group_values = df_friedman[group].dropna().unique()\n",
    "        len_group_columns = len(group_values)\n",
    "        #find the combinations \n",
    "        combinations_ = list(list(itertools.combinations(group_values, len_group_columns)))\n",
    "        res_part_friedman = cal_friedman(df_friedman, eda, group, combinations_, \"participant\")\n",
    "        all_res_friedman.append(res_part_friedman)\n",
    "\n",
    "        #test at the session-level\n",
    "        res_sess_friedman = cal_friedman(df_friedman, eda, group, combinations_, \"session\")\n",
    "        all_res_friedman.append(res_sess_friedman)\n",
    " \n",
    "#saving the matrix as a LaTex table\n",
    "df_friedman_res = pd.concat(all_res_friedman, ignore_index=True)\n",
    "friedman_matrix = make_man_Whithney_U_matrix(df_friedman_res, only_significant=True)\n",
    "saving_matrix_csv_latex(friedman_matrix, file_name =\"Friedman_test\", caption=\"Friedman-test\", label =\"tab:Friedman\" )\n",
    "\n",
    "friedman_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4c600",
   "metadata": {},
   "source": [
    "## Mixed LM\n",
    "\n",
    "This code chapter tests if Market Mechanism (MM) have an impact on EDA signals.\n",
    "\n",
    "Model 1: EDA ~ MM \n",
    "\n",
    "Model 2: EDA ~ MM * C(transaction)\n",
    "\n",
    "Model 3: EDA ~ MM * C(window-type)\n",
    "\n",
    "Model 4: EDA ~ MM * (transaction) * C(window-type)\n",
    "\n",
    "Model 5: EDA ~ Risk\n",
    "\n",
    "Variables that are used (for M1, M2, M3) for broader investigations\n",
    "- \"FV_GUISTY\", \"FV_KIRCHLER\", \"rd_Kirchler_per_round\", \"rad_Kirchler_per_round\", \"rd_Smith_per_round\", \"rad_Smith_per_round\", \"rd_GUISTY_per_round\", \"rad_GUISTY_per_round\",\"V_t\", \"MAD_t\", \"AMAD_t\"\n",
    "-  \"scl_mean\", \"scl_std\", \"scr_peak\", \"scr_auc\", \"scr_per_sec\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NaN values\n",
    "df_mixed_lm = df_all.dropna().copy()\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# standardize signals for LMM:\n",
    "vars_to_z = [ \"FV_GUISTY\", \"FV_KIRCHLER\", \"rd_Kirchler_per_round\", \"rad_Kirchler_per_round\", \"rd_Smith_per_round\", \"rad_Smith_per_round\", \"rd_GUISTY_per_round\", \"rad_GUISTY_per_round\",\"V_t\", \"MAD_t\", \"AMAD_t\"]\n",
    "\n",
    "df_mixed_lm[vars_to_z] = df_mixed_lm[vars_to_z].apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb04e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_eda(formula: str):\n",
    "    \"\"\" Split the string on ~. \"\"\"\n",
    "    lhs, rhs = [s.strip() for s in formula.split(\"~\", 1)]\n",
    "    return lhs\n",
    "\n",
    "def extraction_values_based_of_summary_fixed_effects(model, list_of_results, formula, model_name, market_mechanism):\n",
    "    \"\"\" Extract fixed and interaction effects from the LMM summary model \"\"\"\n",
    "    # Extract fixed-effect coefficients from the model\n",
    "    fixed_effects = model.fe_params\n",
    "    # Extract standard errors of the fixed effects\n",
    "    base_effects = model.bse_fe\n",
    "    # Compute z-values (coefficient / standard error)\n",
    "    zvals = fixed_effects / base_effects\n",
    "    # Extract p-values corresponding to the fixed effects\n",
    "    pvals = model.pvalues.loc[fixed_effects.index]\n",
    "    eda = parse_eda(formula)\n",
    "    mech_pattern = rf\"(?:\\b{re.escape(market_mechanism)}\\b)\"\n",
    "    rows = []\n",
    "    for term in fixed_effects.index:\n",
    "        row = {\n",
    "            \"model_name\": model_name,\n",
    "            \"formula\": formula,\n",
    "            \"eda\": eda,\n",
    "            \"mechanism\": market_mechanism,\n",
    "            \"term\": term,\n",
    "            \"coef\": float(fixed_effects.loc[term]),\n",
    "            \"std_err\": float(base_effects.loc[term]),\n",
    "            \"z_value\": float(zvals.loc[term]),\n",
    "            \"p_value\": float(pvals.loc[term]),\n",
    "            \"is_interaction\": (\":\" in term),\n",
    "            \"involves_mechanism\": bool(re.search(mech_pattern, term)),\n",
    "            \"aic\": getattr(model, \"aic\", None),\n",
    "            \"bic\": getattr(model, \"bic\", None),\n",
    "            \"llf\": getattr(model, \"llf\", None),\n",
    "            \"nobs\": getattr(model, \"nobs\", None)\n",
    "        }\n",
    "        # Adding values to a list \n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def extraction_significant_results(dataframe_results, alpha=0.05, only_main_effects=False):\n",
    "    \"\"\"\n",
    "        Filter and extract significant fixed-effect terms from a results Dataframe.\n",
    "        Extract only significant fixed effects if only_main_effects is true.\n",
    "    \"\"\"\n",
    "    df = dataframe_results.copy()\n",
    "    # Check if Dataframe is empty\n",
    "    if df.empty:\n",
    "        return df\n",
    "    # Create column for significance: True if  p-value < 0.05\n",
    "    df[\"significant\"] = df[\"p_value\"] < alpha\n",
    "    # Filter significance results\n",
    "    df = df[df[\"significant\"] == True]\n",
    "    if only_main_effects:\n",
    "        #creating masks for non main-effects variables\n",
    "        mask_intercept = df[\"term\"].str.contains(\"Intercept\")\n",
    "        mask_c = (\n",
    "            df[\"term\"].str.contains(\"C\\\\(transaction\\\\)\") |\n",
    "            df[\"term\"].str.contains(\"C\\\\(window_type\\\\)\")\n",
    "\n",
    "        )\n",
    "        mask_mech = df[\"involves_mechanism\"]\n",
    "        # Keep rows that are not intercepts and not categorical or the term involves a mechanism\n",
    "        df = df[~mask_intercept & (~mask_c | mask_mech)]\n",
    "\n",
    "    #sorting values\n",
    "    df = df.sort_values([\"formula\", \"p_value\"])\n",
    "    return df\n",
    "\n",
    "def make_effect_pivot_table(df_summary):\n",
    "    \"\"\"\n",
    "        Create a formatted pivot table summarizing effects sizes, standard errors, and significance levels across different models.\n",
    "    \"\"\"\n",
    "    df = df_summary.copy()\n",
    "    # Add stars for significance (*** = 0.005. ** = 0.01)\n",
    "    df[\"stars\"] = df[\"p_value\"].apply(significance_marks)\n",
    "    # Create cell of the matrix by combining the effect string with the significance stars.\n",
    "    df[\"effect\"] = df.apply(lambda r: f\"{r['coef']:.3f} ({r['std_err']:.3f}){r['stars']}\", axis=1)\n",
    "    #Create absolute coeff so that the strongest effect can be displayed in the matrix\n",
    "    df[\"abs_coef\"] = df[\"coef\"].abs()\n",
    "    df = df.sort_values(by=\"abs_coef\", ascending=False)\n",
    "    # Create pivot table: Index = EDA and mechanism, columns reoresenting the model names (e.g. Model 1, Model 2, Model 3)\n",
    "    pivot = df.pivot_table(index=[\"eda\", \"mechanism\"], columns=\"model_name\", values=\"effect\", aggfunc=lambda x: x.iloc[0] ).fillna(\"-\")\n",
    "    #return matrix\n",
    "    return pivot.reset_index()\n",
    "\n",
    "\n",
    "def shorten_variable(name):\n",
    "    \"\"\"\n",
    "        Shorten the formular varibale \n",
    "        rad_* -> rad\n",
    "        rd_* -> rd\n",
    "        C(transaction)[T.] -> Trans[]\n",
    "        C(window_type)[T.] -> Win[]\n",
    "        pre_decision     -> pre_d\n",
    "        during_feedback  -> during_f\n",
    "        post_feedback.   -> post_f\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    name = re.sub(r\"C\\(window_type\\)\\[T\\.([^\\]]+)\\]\", r\"Win[\\1]\", name)\n",
    "    name = re.sub(r\"C\\(transaction\\)\\[T\\.([^\\]]+)\\]\", r\"Tran[\\1]\", name)\n",
    "\n",
    "    name = name.replace(\"C(transaction)\", \"Tran\")\n",
    "    name = name.replace(\"C(window_type)\", \"Win\")\n",
    "\n",
    "    name = name.replace(\"pre_decision\", \"pre_d\")\n",
    "    name = name.replace(\"during_feedback\", \"df\")\n",
    "    name = name.replace(\"post_feedback\", \"pf\")\n",
    "\n",
    "    name = name.replace(\"rd_Kirchler_per_round\", \"rd_Kirchler\")\n",
    "    name = name.replace(\"rad_Kirchler_per_round\", \"rad_Kirchler\")\n",
    "    name = name.replace(\"rd_GUISTY_per_round\", \"rd_Giusti\")\n",
    "    name = name.replace(\"rad_GUISTY_per_round\", \"rad_Giusti\")\n",
    "    name = name.replace(\"rd_Smith_per_round\", \"rd_Smith\")\n",
    "    name = name.replace(\"rad_Smith_per_round\", \"rad_Smith\")\n",
    "\n",
    "    name = re.sub(r\"C\\(Risk_Guisty\\)\\[T\\.([^\\]]+)\\]\", r\"Risk_G[\\1]\", name)\n",
    "    name = re.sub(r\"C\\(Risk_Smith\\)\\[T\\.([^\\]]+)\\]\", r\"Risk_S[\\1]\", name)\n",
    "    name = re.sub(r\"C\\(Risk_Kirchler\\)\\[T\\.([^\\]]+)\\]\", r\"Risk_K[\\1]\", name)\n",
    "    name = re.sub(r\"C\\(Vol_Classification_Global\\)\\[T\\.([^\\]]+)\\]\", r\"Risk_Vol_G[\\1]\", name)\n",
    "\n",
    "    name = re.sub(r\"C\\(Risk_Guisty\\)\", r\"Risk_G\", name)\n",
    "    name = re.sub(r\"C\\(Risk_Smith\\)\", r\"Risk_S\", name)\n",
    "    name = re.sub(r\"C\\(Risk_Kirchler\\)\", r\"Risk_K\", name)\n",
    "    name = re.sub(r\"C\\(Vol_Classification_Global\\)\", r\"Risk_Vol_G\", name)\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def saving_summary_of_model(summary, path, latex_lable, latex_caption):\n",
    "    \"\"\"\n",
    "        Saving summary of the LMM model in latex\n",
    "    \"\"\"\n",
    "\n",
    "    table = summary.tables[1]\n",
    "\n",
    "    df_summary = table.reset_index()\n",
    "\n",
    "    df_summary.rename(columns= {\"index\": \"variable\"}, inplace=True)\n",
    "    #shorting the term variables with the method: shorten_variable\n",
    "    df_summary[\"term\"] = df_summary[\"variable\"].apply(shorten_variable)\n",
    "\n",
    "    # drop the Confidential interval -> summary to big\n",
    "    df_summary = df_summary.drop(\n",
    "        columns=[col for col in df_summary.columns if \"0.025\" in col or \"0.975\" in col],\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    # renaming columns\n",
    "    df_summary = df_summary[[\"term\", \"Coef.\", \"Std.Err.\", \"z\", \"P>|z|\"]].rename(columns={\n",
    "        \"Coef.\": \"Estimate\",\n",
    "        \"Std.Err.\": \"Std. Error\",\n",
    "        \"P>|z|\": \"p-value\"\n",
    "    })\n",
    "\n",
    "    # creating latex code based of the sumamry\n",
    "    latex_code = df_summary.to_latex(\n",
    "        index=False,\n",
    "        float_format=\"%.3f\",\n",
    "        caption=latex_caption,\n",
    "        label=latex_lable\n",
    "    )\n",
    "\n",
    "    #saving the latex code\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(latex_code)\n",
    "\n",
    "    if (path.exists()):\n",
    "        print(f\"saved in {path}\")\n",
    "    else: print(f\"Could not be saved.\")\n",
    "\n",
    "\n",
    "def saving_matrix_csv_latex(matrix, file_name, caption, label, FOLDER = TESTS_RESULT):\n",
    "    \"\"\"\n",
    "        Saving the matrix of the results in csv and latex format\n",
    "    \"\"\"\n",
    "\n",
    "    #check if matrix table is none\n",
    "    if matrix is None:\n",
    "        raise ValueError(\"No matrix available\")\n",
    "\n",
    "    #saving path\n",
    "    tex_path = FOLDER / f\"{file_name}.tex\"\n",
    "    csv_path = FOLDER / f\"{file_name}.csv\"\n",
    "    matrix.to_csv(csv_path, index=False, encoding=\"utf-8\", sep=\";\")\n",
    "\n",
    "    with open(tex_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(matrix.to_latex(\n",
    "            index=False,\n",
    "            escape=True,\n",
    "            float_format=\"%.3f\",  \n",
    "            caption=f\"{caption}\",\n",
    "            label=f\"{label}\",\n",
    "            column_format=\"l\" + \"r\" * (len(matrix.columns) - 1)  \n",
    "        ))\n",
    "\n",
    "\n",
    "    print(f\"‚úÖ Exported Tables:\\n- {tex_path}\\n- {csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0083d",
   "metadata": {},
   "source": [
    "### Overview Model 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b85f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applying the Linear mixed Model test. Goal: Investigate the influence of Market Mechanims during Decision phases or with transaction on the EDA signal.\"\n",
    "- Model 1: \"M1_General\", \"formula\" : EDA ~ market_mechanism\" -> Influence of RD, Vol on EDA\n",
    "- Model 2: \"M2_MM_Transaction\", \"formula\" :EDA ~ {market_mechanism} * C(transaction)\" ->  Influence of RD, Vol on EDA with different transaction types\n",
    "- Model 3: \"M3_MM_WindowType\", \"formula\": EDA~ {market_mechanism} * C(window_type)\"} -> Influence of RD, Vol on EDA with transaction  during different decision-phases\n",
    "\n",
    "Printing the result\n",
    "Summarize the results of the 3 models with different variable combinations\n",
    "Identify which combination has a significant results with a coeff > 0.1\n",
    "Based on this furher investigations\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "summary_all  = []\n",
    "count = 0\n",
    "\n",
    "for eda in [\"scr_auc\", \"scr_amplitude\", \"scr_per_sec\", \"scl_mean\", \"scl_std\"]:\n",
    "    for market_mechanism in [\"FV_GUISTY\", \"FV_KIRCHLER\", \"rd_Kirchler_per_round\", \"rad_Kirchler_per_round\", \"rd_Smith_per_round\", \"rad_Smith_per_round\", \"rd_GUISTY_per_round\", \"rad_GUISTY_per_round\", \"V_t\", \"MAD_t\", \"AMAD_t\"]:\n",
    "        formel = [ \n",
    "            {\"model\": \"M1_General\", \"formula\" :f\"{eda} ~ {market_mechanism}\"},\n",
    "            {\"model\": \"M2_MM_Transaction\", \"formula\" :f\"{eda} ~ {market_mechanism} * C(transaction)\"},\n",
    "            {\"model\": \"M3_MM_WindowType\", \"formula\": f\"{eda} ~ {market_mechanism} * C(window_type)\"}, \n",
    "            \n",
    "            ]\n",
    "        for entry in formel:\n",
    "                count +=1\n",
    "                print(f\"\\n\\n\\n============================================= {entry[\"formula\"]} =============================================\\n\")\n",
    "                #variance component = session\n",
    "                vc = {\"session\": \"0 + C(session)\"}  \n",
    "                model = smf.mixedlm(entry[\"formula\"],\n",
    "                    data=df_mixed_lm,\n",
    "                    groups=df_mixed_lm[\"participant\"], vc_formula=vc,\n",
    "                ).fit(reml= False)  #fitting the model\n",
    "                print(model.summary())\n",
    "                #adding the summary to the list summary_all\n",
    "                summary_all.extend(extraction_values_based_of_summary_fixed_effects(model, summary_all ,entry[\"formula\"], entry[\"model\"], market_mechanism))\n",
    "\n",
    "\n",
    "\n",
    "df_summary = pd.DataFrame(summary_all)     \n",
    "# Extract significiant results\n",
    "df_significant = extraction_significant_results(df_summary, alpha=0.05, only_main_effects=True)\n",
    "#filter results with an effect > 0.05\n",
    "df_bigger_0dot1 = df_significant[abs(df_significant[\"coef\"]) >= 0.05]\n",
    "\n",
    "# shorten variables for a better visualisation, and saving them as a LaTex table\n",
    "df_significant[\"term\"] = df_significant[\"term\"].apply(shorten_variable)\n",
    "df_significant[\"mechanism\"] = df_significant[\"mechanism\"].apply(shorten_variable)\n",
    "result_table = make_effect_pivot_table(df_significant)\n",
    "saving_matrix_csv_latex(result_table, file_name =\"LMM_M1_to_M3_MATRIX\", caption = \"LMM Result Matrix of M1, M2, M3\", label=\"LMM_RESULT_MATRIX\", FOLDER=TESTS_RESULT )\n",
    "\n",
    "\n",
    "print(count)\n",
    "\n",
    "\n",
    "result_table\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a4887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigger_0dot1.sort_values(\"coef\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12354269",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087556ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Filtering the significant results (use the variables who have a coeff >=0.1)\n",
    "    Visualize explizit the  Model 2 f\"{eda} ~ {mechanims} * C(transaction)\" for better understanding\n",
    "\n",
    "\"\"\"\n",
    "results_M2 = []\n",
    "for eda in [\"scr_auc\", \"scl_std\"]:\n",
    "    for mechanims in [\"rd_Kirchler_per_round\", \"rad_Kirchler_per_round\", \"rd_Smith_per_round\", \"rad_Smith_per_round\", \"rd_GUISTY_per_round\", \"rad_GUISTY_per_round\", \"V_t\", \"MAD_t\", \"AMAD_t\"]:\n",
    "        print(eda)\n",
    "        print(mechanims)\n",
    "\n",
    "        #### Model 2\n",
    "        new_formula = f\"{eda} ~ {mechanims} * C(transaction)\"\n",
    "        print(f\"Modell M2:  {eda} und {mechanims}\")\n",
    "        model = smf.mixedlm(new_formula,\n",
    "                                data=df_mixed_lm,\n",
    "                                groups=df_mixed_lm[\"participant\"]).fit(reml=False)\n",
    "        print(model.summary())\n",
    "        results_M2.extend(extraction_values_based_of_summary_fixed_effects(model, results_M2 ,new_formula, \"M2_MM_TRAN\", mechanims))\n",
    "        \n",
    "        #saving the summary of this test\n",
    "        summary = model.summary()\n",
    "        FOLDER_LMM_M2= TESTS_RESULT / \"LMM_M2\"\n",
    "        os.makedirs(FOLDER_LMM_M2, exist_ok=True)\n",
    "        path = FOLDER_LMM_M2 / f\"MLM_MODEL_2_{eda}_{mechanims}.tex\"\n",
    "        \n",
    "        saving_summary_of_model(summary, path=path , latex_lable =f\"tab:mlm_m2_{eda}_{mechanims}\".lower(), latex_caption =\"Coeffizienten of LMM  Model 2 for {eda} and {mechanims}\")\n",
    "\n",
    "df_summary_M2 = pd.DataFrame(results_M2)     \n",
    "# Extract significant results\n",
    "df_significant_M2 = extraction_significant_results(df_summary_M2, alpha=0.05, only_main_effects=True)\n",
    "# Shorten term and mechanism variables\n",
    "df_significant_M2[\"term\"] = df_significant_M2[\"term\"].apply(shorten_variable)\n",
    "df_significant_M2[\"mechanism\"] = df_significant_M2[\"mechanism\"].apply(shorten_variable)\n",
    "df_bigger_0dot1_M2 = df_significant_M2[abs(df_significant_M2[\"coef\"]) >= 0.05]\n",
    "\n",
    "\n",
    "#new result matrix\n",
    "result_table_M2 = make_effect_pivot_table(df_significant_M2)\n",
    "\n",
    "\n",
    "df_bigger_0dot1_M2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58413a37",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Helper function to create readable, generalized term names\n",
    "def clean_term_name(row):\n",
    "    term = str(row[\"term\"])\n",
    "    mech = str(row[\"mechanism\"])\n",
    "\n",
    "    # 1) Intercept\n",
    "    if term == \"Intercept\":\n",
    "        return \"Intercept\"\n",
    "\n",
    "    # 2) Mechanism - Main effect\n",
    "    if term == mech:\n",
    "        return \"Mechanism\"\n",
    "\n",
    "    # 3) Transaction-Level: C(transaction)[T.xxx]\n",
    "    m = re.match(r\"C\\(transaction\\)\\[T\\.(.+)\\]\", term)\n",
    "    if m:\n",
    "        level = m.group(1)\n",
    "        return f\"Transaction_{level}\"\n",
    "\n",
    "    # 4) Interaction Mechanism √ó Transaction\n",
    "    if \"C(transaction)\" in term and mech in term:\n",
    "        m2 = re.search(r\"C\\(transaction\\)\\[T\\.(.+)\\]\", term)\n",
    "        if m2:\n",
    "            level = m2.group(1)\n",
    "            return f\"Mechanism:Transaction_{level}\"\n",
    "        return \"Mechanism:Transaction\"\n",
    "\n",
    "    \n",
    "    return term\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_effect_matrix_significant(df_summary):\n",
    "    \"\"\"\n",
    "        Method for creating a matrix of significant results. \n",
    "    \"\"\"\n",
    "    df = df_summary.copy()\n",
    "\n",
    "    # Create a formatted effect string: \"coef (std_err)\"\n",
    "    df[\"effect\"] = df.apply(\n",
    "        lambda r: f\"{r['coef']:.3f} ({r['std_err']:.3f})\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Keep only significant effects based on a 5% threshold\n",
    "    df[\"significant\"] = df[\"p_value\"] < 0.05\n",
    "\n",
    "    df[\"term_clean\"] = df.apply(clean_term_name, axis=1)\n",
    "\n",
    "    # Display only significant results\n",
    "    df[\"effect_or_blank\"] = df.apply(\n",
    "        lambda r: r[\"effect\"] if r[\"significant\"] else \"-\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Create matrix where the terms are the columns and the values (cells) are coef. effects. \n",
    "    pivot = df.pivot_table(\n",
    "        index=[\"eda\", \"mechanism\"],\n",
    "        columns=\"term_clean\",\n",
    "        values=\"effect_or_blank\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "\n",
    "    # Remove rows where *all* terms are non-significant\n",
    "    pivot = pivot.replace(\"-\", np.nan).dropna(how=\"all\").fillna(\"-\")\n",
    "\n",
    "    return pivot.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_M2[df_summary_M2[\"p_value\"] < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dacdcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatig a matrix based on signidicant results and saving it as a latex table\n",
    "\n",
    "df_summary_M2[df_summary_M2[\"p_value\"] < 0.05]\n",
    "pivot = make_effect_matrix_significant(df_summary_M2)\n",
    "#pivot[\"term\"] = pivot[\"term\"].apply(shorten_variable)\n",
    "pivot[\"mechanism\"] = pivot[\"mechanism\"].apply(shorten_variable)\n",
    "saving_matrix_csv_latex(pivot, \"m2_transaction_matrix_result\", caption = \"M2: EDA ~ MM * C(transaction)\", label = \"tab:M2_trans\", FOLDER = TESTS_RESULT)\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08557ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting effect size heatmap\n",
    "heat = df_significant_M2.copy()\n",
    "heat[\"sign\"] = np.sign(heat[\"coef\"])\n",
    "heat[\"abs_beta\"] = heat[\"coef\"].abs()\n",
    "heat_pivot = heat.pivot_table(index=\"eda\", columns=\"mechanism\", values=\"abs_beta\", aggfunc=\"max\")\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.heatmap(heat_pivot, annot=False, cmap=\"coolwarm\", center=0, vmax=1)\n",
    "plt.title(\"Effect Size Heatmap (Œ≤ across mechanisms and EDAs)\")\n",
    "plt.show()\n",
    "path = TESTS_RESULT / \"Effect_size_heatmap_m2_eda_mm_transaction.png\"\n",
    "fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "if path.exists():\n",
    "    print(f\"Saved: {path}\")\n",
    "else: print(\"Heatmap colud not be saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dae2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of effect coefficients\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_significant_M2, x=\"mechanism\", y=\"coef\", hue=\"eda\", ci=None)\n",
    "plt.axhline(0, color=\"grey\", linestyle=\"--\")\n",
    "plt.title(\"effects (Coefficient) mit p < 0.05 und |coef| > 0.1 im Modell 4\")\n",
    "plt.xlabel(\"market mechanism\")\n",
    "plt.ylabel(\"Coefficient\")\n",
    "plt.legend(title=\"EDA signal\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "path = TESTS_RESULT / \"Effect_size_Barplot_m2_eda_mm_transaction.png\"\n",
    "fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "if path.exists():\n",
    "    print(f\"Saved: {path}\")\n",
    "else: print(\"Heatmap colud not be saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90aa3bb",
   "metadata": {},
   "source": [
    "### Modell 3: EDA ~ MM * C(window-type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Filtering the significant results (use the variables who have a coeff >=0.1)\n",
    "    Visualize explizit the  Model 3 f\"{eda} ~ {mechanims} * C(window_type)\" for better understanding\n",
    "\n",
    "\"\"\"\n",
    "results_M3 = []\n",
    "for eda in [\"scr_auc\", \"scl_std\"]:\n",
    "    for mechanims in [\"rd_Kirchler_per_round\", \"rad_Kirchler_per_round\", \"rd_Smith_per_round\", \"rad_Smith_per_round\", \"rd_GUISTY_per_round\", \"rad_GUISTY_per_round\", \"V_t\", \"MAD_t\", \"AMAD_t\"]:\n",
    "        print(eda)\n",
    "        print(mechanims)\n",
    "\n",
    "        #### Model 2\n",
    "        new_formula = f\"{eda} ~ {mechanims} * C(window_type)\"\n",
    "        print(f\"Modell M3:  {eda} und {mechanims}\")\n",
    "        model = smf.mixedlm(new_formula,\n",
    "                                data=df_mixed_lm,\n",
    "                                groups=df_mixed_lm[\"participant\"]).fit(reml=False)\n",
    "        print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "        results_M3.extend(extraction_values_based_of_summary_fixed_effects(model, results_M3 ,new_formula, \"M3_MM_TRAN\", mechanims))\n",
    "        \n",
    "        #saving the summary of this test\n",
    "        summary = model.summary()\n",
    "        FOLDER_LMM_M3= TESTS_RESULT / \"LMM_M3\"\n",
    "        os.makedirs(FOLDER_LMM_M3, exist_ok=True)\n",
    "        path = FOLDER_LMM_M3 / f\"MLM_MODEL_3_{eda}_{mechanims}.tex\"\n",
    "        \n",
    "        saving_summary_of_model(summary, path=path , latex_lable =f\"tab:mlm_m3_{eda}_{mechanims}\".lower(), latex_caption =\"Coeffizienten of LMM  Model 3 for {eda} and {mechanims}\")\n",
    "# Creating Dataframe\n",
    "df_summary_M3 = pd.DataFrame(results_M3)\n",
    "# Ectracting singnificant results     \n",
    "df_significant_M3 = extraction_significant_results(df_summary_M3, alpha=0.05, only_main_effects=True)\n",
    "# Shorting variables\n",
    "df_significant_M3[\"term\"] = df_significant_M3[\"term\"].apply(shorten_variable)\n",
    "df_significant_M3[\"mechanism\"] = df_significant_M3[\"mechanism\"].apply(shorten_variable)\n",
    "df_bigger_0dot1_M3 = df_significant_M3[abs(df_significant_M3[\"coef\"]) >= 0.05]\n",
    "\n",
    "\n",
    "#new result matrix\n",
    "result_table_M3 = make_effect_pivot_table(df_bigger_0dot1_M3)\n",
    "\n",
    "\n",
    "df_summary_M3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a37a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_M3[(df_summary_M3[\"p_value\"]<0.05) &(df_summary_M3[\"term\"] != \"Intercept\") ][[\"formula\", \"mechanism\", \"coef\", \"std_err\", \"p_value\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65391846",
   "metadata": {},
   "source": [
    "### Model 4: EDA ~ MM * C(transaction) * C(window-type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applying the Linear mixed Model test. Goal: Invertsigate the influence of Market Mechanims during multiple decision phases and with transaction on the EDA signal.\"\n",
    "- Model 4: \"M4_Trans_Window\", \"formula\" : \"{eda} ~ {mechanims} * C(transaction) * C(window_type)\" -> Influence of RD, Vol during a decision phases with transaction on EDA\n",
    "\n",
    "Using the variables of the significant extraction\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "results_M4 = []\n",
    "for eda in [\"scr_auc\", \"scl_std\"]:\n",
    "    for mechanims in [\"rd_Kirchler_per_round\", \"rad_Kirchler_per_round\", \"rd_Smith_per_round\", \"rad_Smith_per_round\", \"rd_GUISTY_per_round\", \"rad_GUISTY_per_round\", \"V_t\", \"MAD_t\", \"AMAD_t\"]:\n",
    "        print(eda)\n",
    "        print(mechanims)\n",
    "        #executing modell test 4\n",
    "        new_formula = f\"{eda} ~ {mechanims} * C(transaction) * C(window_type)\"\n",
    "        print(f\"Modell M4:  {eda} und {mechanims}\")\n",
    "        model = smf.mixedlm(new_formula,\n",
    "                                data=df_mixed_lm,\n",
    "                                groups=df_mixed_lm[\"participant\"]).fit(reml=False)\n",
    "        print(model.summary())\n",
    "        results_M4.extend(extraction_values_based_of_summary_fixed_effects(model, results_M4 ,new_formula, \"M4_MM_TRAN_DEC\", mechanims))\n",
    "\n",
    "        #Saving each summary \n",
    "        summary = model.summary()\n",
    "        FOLDER_LMM_M4= TESTS_RESULT / \"LMM_M4\"\n",
    "        os.makedirs(FOLDER_LMM_M4, exist_ok=True)\n",
    "        path = FOLDER_LMM_M4 / f\"MLM_MODEL_4_{eda}_{mechanims}.tex\"\n",
    "        \n",
    "        saving_summary_of_model(summary, path=path , latex_lable =f\"tab:mlm_m4_{eda}_{mechanims}\".lower(), latex_caption =\"Coeffizienten of LMM for {eda} and {mechanims}\")\n",
    "\n",
    "\n",
    "\n",
    "df_summary_M4 = pd.DataFrame(results_M4) \n",
    "# Extracting significant variables\n",
    "df_significant_M4 = extraction_significant_results(df_summary_M4, alpha=0.05, only_main_effects=True)\n",
    "# Shorten variables\n",
    "df_significant_M4[\"term\"] = df_significant_M4[\"term\"].apply(shorten_variable)\n",
    "df_significant_M4[\"mechanism\"] = df_significant_M4[\"mechanism\"].apply(shorten_variable)\n",
    "#Create matrix\n",
    "matrix_M4 = make_effect_matrix_significant(df_significant_M4)   \n",
    "df_bigger_0dot1_M4 = df_significant_M4[abs(df_significant_M4[\"coef\"]) >= 0.1]\n",
    "\n",
    "#saving Results in latex and csv\n",
    "path_latex = FOLDER_LMM_M4 / f\"MLM_MODEL_4_RESULTS.tex\"\n",
    "latex = df_bigger_0dot1_M4.to_latex(index=False, float_format=\"%.3f\", caption=\"Model 4: EDA ~ MM * Transaction * Dec\", label=\"tab:LMM_model4\")\n",
    "with open(path_latex, \"w\") as f:\n",
    "    f.write(latex)\n",
    "path_csv = FOLDER_LMM_M4 / f\"MLM_MODEL_4_RESULTS.csv\"\n",
    "df_bigger_0dot1_M4.to_excel(path_csv, index=False)\n",
    "\n",
    "df_significant_M4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separatung EDA table into two tables (SCL and SCRs)\n",
    "\n",
    "x = df_significant_M4[[\"eda\", \"term\", \"coef\", \"std_err\", \"z_value\", \"p_value\"]]\n",
    "x_scl = x[x[\"eda\"]== \"scl_std\"].round(3)\n",
    "x_scr = x[x[\"eda\"]== \"scr_auc\"].round(3)\n",
    "\n",
    "path_latex = FOLDER_LMM_M4 / f\"MLM_MODEL_4_SIGN_SCL.tex\"\n",
    "latex = x_scl.to_latex(index=False, float_format=\"%.3f\", caption=\"Model 4: SCL ~ MM * Transaction * Dec\", label=\"tab:LMM_model4_SCL\", escape=True)\n",
    "with open(path_latex, \"w\") as f:\n",
    "    f.write(latex)\n",
    "if path_latex.exists():\n",
    "    print(path_latex)\n",
    "\n",
    "path_latex = FOLDER_LMM_M4 / f\"MLM_MODEL_4_SIGN_SCR.tex\"\n",
    "latex = x_scr.to_latex(index=False, float_format=\"%.3f\", caption=\"Model 4: SCR ~ MM * Transaction * Dec\", label=\"tab:LMM_model4_SCR\", escape=True)\n",
    "with open(path_latex, \"w\") as f:\n",
    "    f.write(latex)\n",
    "if path_latex.exists():\n",
    "    print(path_latex)\n",
    "\n",
    "\n",
    "x_scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#visualize effectsize with barplot\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_significant_M4, x=\"mechanism\", y=\"coef\", hue=\"eda\", ci=None)\n",
    "plt.axhline(0, color=\"grey\", linestyle=\"--\")\n",
    "plt.title(\"Effekte (Koeffizienten) mit p < 0.05 und |coef| > 0.1 im Modell 4\")\n",
    "plt.xlabel(\"Marktmechanismus\")\n",
    "plt.ylabel(\"Koeffizient\")\n",
    "plt.legend(title=\"EDA-Signal\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "path = FOLDER_LMM_M4 / \"barchart_m4_effectsize.png\"\n",
    "fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e223890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize effectsize with heatmap\n",
    "heat = df_significant_M4.copy()\n",
    "heat[\"sign\"] = np.sign(heat[\"coef\"])\n",
    "heat[\"abs_beta\"] = heat[\"coef\"].abs()\n",
    "heat_pivot = heat.pivot_table(index=\"eda\", columns=\"mechanism\", values=\"abs_beta\", aggfunc=\"max\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(heat_pivot, annot=False, cmap=\"coolwarm\", center=0, vmax=1)\n",
    "plt.title(\"Effect Size Heatmap (Œ≤ across mechanisms and EDAs)\")\n",
    "plt.show()\n",
    "path = FOLDER_LMM_M4 / \"heatmap_m4_effectsize.png\"\n",
    "fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732da31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigger_0dot1_M4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac6d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(row):\n",
    "    \"\"\"\n",
    "        Classify term variables and Mechanism into categories\n",
    "        Goal: For a better visualization\n",
    "    \"\"\"\n",
    "    term = str(row[\"term\"])\n",
    "    mech = str(row[\"mechanism\"])\n",
    "\n",
    "    if term == \"Intercept\":\n",
    "        return \"Intercept\"\n",
    "    if term == mech:\n",
    "        return \"Mechanism\"\n",
    "    if \":\" in term and mech in term:\n",
    "        return \"Mechanism:Vol\"\n",
    "    if \":\" not in term and term != mech:\n",
    "        return \"Volatility\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def summarize_mechanism_volatility(df):\n",
    "    \"\"\"\n",
    "        Creating a a matrix based on a dataframe with the results.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Adding significance mareks to coefficient value\n",
    "    df[\"stars\"] = df[\"p_value\"].apply(significance_marks)\n",
    "    # Combine coeef value with star marks\n",
    "    df[\"eff\"] = df.apply(lambda r: f\"{r['coef']:.3f} ({r['std_err']:.3f}){r['stars']}\", axis=1)\n",
    "    df[\"sig\"] = df[\"p_value\"] < 0.05\n",
    "\n",
    "    # Classify term type into categories \n",
    "    df[\"term_type\"] = df.apply(classify, axis=1)\n",
    "\n",
    "\n",
    "    df = df[df[\"term_type\"].notna()].copy()\n",
    "\n",
    "\n",
    "    # Keep sifnificant resulzs\n",
    "    df[\"value\"] = df.apply(lambda r: r[\"eff\"] if r[\"sig\"] else \"-\", axis=1)\n",
    "\n",
    "    # Create matrix\n",
    "    pivot = df.pivot_table(\n",
    "        index=[\"formula\"],\n",
    "        columns=\"term_type\",\n",
    "        values=\"value\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "\n",
    "    # Ensure all expected columns exist (fill missing ones with \"-\")\n",
    "    for col in [\"Intercept\", \"Mechanism\", \"Volatility\", \"Mechanism:Vol\"]:\n",
    "        if col not in pivot.columns:\n",
    "            pivot[col] = \"-\"\n",
    "\n",
    "    # Order the columns in a logical structure\n",
    "    pivot = pivot[[\"Intercept\", \"Mechanism\", \"Volatility\", \"Mechanism:Vol\"]]\n",
    "\n",
    "    return pivot.reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d41545a",
   "metadata": {},
   "source": [
    "### Model 5: EDA ~ FV_D * Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce69da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Investigating if there is an interaction effect  between rd/rad and volatility on EDA\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result_model_5 = []\n",
    "for eda in [\"scr_auc\", \"scl_std\"]:\n",
    "\n",
    "    for market_mechanism in [\"rd_Kirchler_per_round\", \"rad_Kirchler_per_round\",\"rad_Smith_per_round\", \"rd_GUISTY_per_round\", \"rad_GUISTY_per_round\", \"rd_Smith_per_round\"]:\n",
    "        for vol in [\"V_t\", \"MAD_t\", \"AMAD_t\"]:   \n",
    "            formel = f\"{eda} ~ {market_mechanism} * {vol}\"\n",
    "            \n",
    "            print(f\"\\n\\n\\n============================================= {formel} =============================================\\n\")\n",
    "            vc = {\"session\": \"0 + C(session)\"}  \n",
    "            model = smf.mixedlm(formel,\n",
    "                    data=df_mixed_lm,\n",
    "                    groups=df_mixed_lm[\"participant\"], vc_formula=vc,\n",
    "                ).fit(reml= False)\n",
    "            print(model.summary())\n",
    "            result_model_5.extend(extraction_values_based_of_summary_fixed_effects(model, result_model_5 ,formel, \"M5_VOL_MM\", market_mechanism))\n",
    "\n",
    "df_summary_M5 = pd.DataFrame(result_model_5)\n",
    "# Shorten variables\n",
    "df_summary_M5[\"term\"] = df_summary_M5[\"term\"].apply(shorten_variable)\n",
    "df_summary_M5[\"mechanism\"] = df_summary_M5[\"mechanism\"].apply(shorten_variable)  \n",
    "df_summary_M5[\"formula\"] = df_summary_M5[\"formula\"].apply(shorten_variable)  \n",
    "# Extract significant variables\n",
    "df_significant_M5 = extraction_significant_results(df_summary_M5, alpha=0.05, only_main_effects=True)\n",
    "df_significant_M5[abs(df_significant_M5[\"coef\"]) > 0.05]\n",
    "# Create matrix\n",
    "matrix_m5 = summarize_mechanism_volatility(df_summary_M5)\n",
    "\n",
    "\n",
    "#saving Results in latex and csv\n",
    "path_latex = TESTS_RESULT / f\"MLM_MODEL_5_RESULTS.tex\"\n",
    "latex = df_significant_M5.to_latex(index=False, float_format=\"%.3f\", caption=\"Model 5: EDA ~ RD * Vol\", label=\"tab:LMM_model5\")\n",
    "with open(path_latex, \"w\") as f:\n",
    "    f.write(latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86512de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_significant_M5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sava matrix as a LATEX file\n",
    "path_latex = TESTS_RESULT / f\"MLM_MODEL_5_SIGN_Matrix.tex\"\n",
    "latex = matrix_m5.to_latex(index=False, float_format=\"%.3f\", caption=\"Model 5: EDA ~ FV * VOL\", label=\"tab:LMM_model5_Matrix\", escape=True)\n",
    "with open(path_latex, \"w\") as f:\n",
    "    f.write(latex)\n",
    "if path_latex.exists():\n",
    "    print(path_latex)\n",
    "\n",
    "matrix_m5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_M5[(df_summary_M5[\"p_value\"] < 0.05) & (df_summary_M5[\"term\"] != \"Intercept\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0cac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize effectsize with heatmap\n",
    "heat = df_significant_M5.copy()\n",
    "heat[\"sign\"] = np.sign(heat[\"coef\"])\n",
    "heat[\"abs_beta\"] = heat[\"coef\"].abs()\n",
    "heat_pivot = heat.pivot_table(index=\"eda\", columns=\"mechanism\", values=\"abs_beta\", aggfunc=\"max\")\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.heatmap(heat_pivot, annot=False, cmap=\"coolwarm\", center=0, vmax=0.25)\n",
    "plt.title(\"Effect Size Heatmap (Œ≤ across mechanisms and EDAs)\")\n",
    "plt.show()\n",
    "path = TESTS_RESULT / \"heatmap_m5_effectsize.png\"\n",
    "fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806dd930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize effectsize with barplot\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_significant_M5, x=\"mechanism\", y=\"coef\", hue=\"eda\", ci=None)\n",
    "plt.axhline(0, color=\"grey\", linestyle=\"--\")\n",
    "plt.title(\"Effekte (Koeffizienten) mit p < 0.05 und |coef| > 0.1 im Modell 5\")\n",
    "plt.xlabel(\"Marktmechanismus\")\n",
    "plt.ylabel(\"Koeffizient\")\n",
    "plt.legend(title=\"EDA-Signal\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "path = TESTS_RESULT / \"barplot_m5_effectsize.png\"\n",
    "fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b88f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 5B: EDA ~ FV_D * Volatility * C(transaction) * C(window-type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Investigating if there is an interaction effect  between rd/rad and volatility on EDA\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result_model_5_b = []\n",
    "for eda in [\"scr_auc\", \"scl_std\"]:\n",
    "\n",
    "    for market_mechanism in [\"rd_Kirchler_per_round\", \"rad_Kirchler_per_round\"]:\n",
    "        for vol in [\"V_t\", \"MAD_t\", \"AMAD_t\"]:   \n",
    "            formel = f\"{eda} ~ {market_mechanism} * {vol} * C(transaction) * C(window_type) \"\n",
    "            \n",
    "            print(f\"\\n\\n\\n============================================= {formel} =============================================\\n\")\n",
    "            vc = {\"session\": \"0 + C(session)\"}  \n",
    "            model = smf.mixedlm(formel,\n",
    "                    data=df_mixed_lm,\n",
    "                    groups=df_mixed_lm[\"participant\"], vc_formula=vc,\n",
    "                ).fit(reml= False)\n",
    "            print(model.summary())\n",
    "            result_model_5_b.extend(extraction_values_based_of_summary_fixed_effects(model, result_model_5_b ,formel, \"M5b_VOL_MM\", market_mechanism))\n",
    "\n",
    "df_summary_M5_b = pd.DataFrame(result_model_5_b)\n",
    "#shorten variables\n",
    "df_summary_M5_b[\"term\"] = df_summary_M5_b[\"term\"].apply(shorten_variable)\n",
    "df_summary_M5_b[\"mechanism\"] = df_summary_M5_b[\"mechanism\"].apply(shorten_variable)  \n",
    "df_summary_M5_b[\"formula\"] = df_summary_M5_b[\"formula\"].apply(shorten_variable)  \n",
    "#extract significant results\n",
    "df_significant_M5_b = extraction_significant_results(df_summary_M5_b, alpha=0.05, only_main_effects=True)\n",
    "\n",
    "\n",
    "saving = df_significant_M5_b[df_significant_M5_b[\"p_value\"] < 0.05][[\"formula\", \"term\", \"coef\", \"std_err\", \"p_value\"]]\n",
    "#saving Results in latex and csv\n",
    "path_latex = TESTS_RESULT / f\"MLM_MODEL_5_RESULTS_significant.tex\"\n",
    "latex = saving.to_latex(index=False, float_format=\"%.3f\", caption=\"Model 5b: EDA ~ RD * Vol * C(transaction) * C(window_type)\", label=\"tab:LMM_model5b\",escape=True)\n",
    "with open(path_latex, \"w\") as f:\n",
    "    f.write(latex)\n",
    "if path_latex.exists():\n",
    "    print(path_latex)\n",
    "path_csv = TESTS_RESULT / f\"MLM_MODEL_5_RESULTS_significant.csv\"\n",
    "saving.to_excel(path_csv, index=False)\n",
    "\n",
    "saving\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c13fc3",
   "metadata": {},
   "source": [
    "### EDA ~ Risk\n",
    "\n",
    "- 1. EDA ~ Risk\n",
    "- 2. EDA ~ Risk * C(transaction)\n",
    "- 3. EDA ~ Risk * C(window-type)\n",
    "- 4. EDA ~ Risk * C(transaction) * C(window-type)\n",
    "\n",
    "Risk variables: [\"Risk_Guisty\", \"Risk_Kirchler\", \"Vol_Classification_Global\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74fc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Testing if Risk categeories can influence the EDA-Signal (using scr_auc and scl_std)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result_M6=[]\n",
    "for eda in [\"scr_auc\", \"scl_std\"]:\n",
    "    for RISK in [\"Risk_Guisty\", \"Risk_Kirchler\", \"Vol_Classification_Global\"]:\n",
    "\n",
    "        formel = f\"{eda} ~ C({RISK})\"\n",
    "                    \n",
    "        print(f\"\\n\\n\\n============================================= {formel} =============================================\\n\")\n",
    "        vc = {\"session\": \"0 + C(session)\"}  \n",
    "        model = smf.mixedlm(formel,\n",
    "                            data=df_mixed_lm,\n",
    "                            groups=df_mixed_lm[\"participant\"], vc_formula=vc,\n",
    "                        ).fit(reml= False)\n",
    "        print(model.summary())\n",
    "        result_M6.extend(extraction_values_based_of_summary_fixed_effects(model, result_M6 ,formel, \"M6_EDA_RISK\", RISK))\n",
    "\n",
    "        formel = f\"{eda} ~ C({RISK}) * C(transaction)\"\n",
    "                    \n",
    "        print(f\"\\n\\n\\n============================================= {formel} =============================================\\n\")\n",
    "        vc = {\"session\": \"0 + C(session)\"}  \n",
    "        model = smf.mixedlm(formel,\n",
    "                            data=df_mixed_lm,\n",
    "                            groups=df_mixed_lm[\"participant\"], vc_formula=vc,\n",
    "                        ).fit(reml= False)\n",
    "        print(model.summary())\n",
    "        result_M6.extend(extraction_values_based_of_summary_fixed_effects(model, result_M6 ,formel, \"M6_EDA_RISK_transaction\", RISK))\n",
    "\n",
    "        formel = f\"{eda} ~ C({RISK}) * C(window_type)\"\n",
    "                    \n",
    "        print(f\"\\n\\n\\n============================================= {formel} =============================================\\n\")\n",
    "        vc = {\"session\": \"0 + C(session)\"}  \n",
    "        model = smf.mixedlm(formel,\n",
    "                            data=df_mixed_lm,\n",
    "                            groups=df_mixed_lm[\"participant\"], vc_formula=vc,\n",
    "                        ).fit(reml= False)\n",
    "        print(model.summary())\n",
    "\n",
    "        result_M6.extend(extraction_values_based_of_summary_fixed_effects(model, result_M6 ,formel, \"M7_EDA_RISK_window\", RISK))\n",
    "\n",
    "        formel = f\"{eda} ~ C({RISK}) * C(transaction) * C(window_type)\"\n",
    "\n",
    "        print(f\"\\n\\n\\n============================================= {formel} =============================================\\n\")\n",
    "        vc = {\"session\": \"0 + C(session)\"}  \n",
    "        model = smf.mixedlm(formel,\n",
    "                            data=df_mixed_lm,\n",
    "                            groups=df_mixed_lm[\"participant\"], vc_formula=vc,\n",
    "                        ).fit(reml= False)\n",
    "        print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "df_summary_M6 = pd.DataFrame(result_M6)  \n",
    "df_summary_M6[\"term\"] = df_summary_M6[\"term\"].apply(shorten_variable) \n",
    "df_summary_M6[\"formula\"] = df_summary_M6[\"formula\"].apply(shorten_variable) \n",
    "df_significant_M6 = extraction_significant_results(df_summary_M6, alpha=0.05, only_main_effects=True)\n",
    "\n",
    "# Saving matrix table\n",
    "result_table_M6 = make_effect_pivot_table(df_significant_M6)\n",
    "path = TESTS_RESULT / \"Modell_Risk_Category_Results\"\n",
    "saving_matrix_csv_latex(result_table_M6, file_name=path, caption = \"LMM - EDA ~¬†Risk Categorization\", label = \"tab:LMM_RISK_Categorization\", FOLDER = TESTS_RESULT)\n",
    "result_table_M6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving matrix table\n",
    "x_M6= df_summary_M6[(df_summary_M6[\"p_value\"] < 0.05) & (df_summary_M6[\"term\"] != \"Intercept\")][[\"formula\",\"term\", \"coef\", \"std_err\", \"p_value\"]].sort_values([\"formula\"])\n",
    "path = TESTS_RESULT / \"Modell_Risk_Results\"\n",
    "saving_matrix_csv_latex(x_M6, file_name=path, caption = \"LMM - EDA ~¬†Risk Categorization\", label = \"tab:LMM_RISK_Categorization\", FOLDER = TESTS_RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b004cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize effectsize with heatmap\n",
    "heat = df_summary_M6.copy()\n",
    "heat[\"sign\"] = np.sign(heat[\"coef\"])\n",
    "heat[\"abs_beta\"] = heat[\"coef\"].abs()\n",
    "heat_pivot = heat.pivot_table(index=\"eda\", columns=\"mechanism\", values=\"abs_beta\", aggfunc=\"max\")\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.heatmap(heat_pivot, annot=False, cmap=\"coolwarm\", center=0, vmax=1)\n",
    "plt.title(\"Effect Size Heatmap (Œ≤ across mechanisms and EDAs)\")\n",
    "plt.show()\n",
    "path = TESTS_RESULT / \"heatmap_m_risk_classification_effectsize.png\"\n",
    "fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize effectsize with barplot\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_summary_M6, x=\"mechanism\", y=\"coef\", hue=\"eda\", ci=None)\n",
    "plt.axhline(0, color=\"grey\", linestyle=\"--\")\n",
    "plt.title(\"Effekte (Koeffizienten) mit p < 0.05 und |coef| > 0.1 im Modell 2\")\n",
    "plt.xlabel(\"Marktmechanismus\")\n",
    "plt.ylabel(\"Koeffizient\")\n",
    "plt.legend(title=\"EDA-Signal\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "path = TESTS_RESULT / \"barplot_m_risk_classification_effectsize.png\"\n",
    "fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d196c",
   "metadata": {},
   "source": [
    "### EDA ~ Risk * C(Vol_Classification_Global)\n",
    "\n",
    "\n",
    "\n",
    "Risk variables: [\"Risk_Guisty\", \"Risk_Kirchler\"]\n",
    "\n",
    "EDA variables: [\"scr_auc\", \"scl_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23139515",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_M11=[]\n",
    "for eda in [\"scr_auc\", \"scl_std\"]:\n",
    "    for RISK in [\"Risk_Guisty\", \"Risk_Kirchler\"]:\n",
    "        formel = f\"{eda} ~ C({RISK}) * C(Vol_Classification_Global)\" \n",
    "                    \n",
    "        print(f\"\\n\\n\\n============================================= {formel} =============================================\\n\")\n",
    "        vc = {\"session\": \"0 + C(session)\"}  \n",
    "        model = smf.mixedlm(formel,\n",
    "                            data=df_mixed_lm,\n",
    "                            groups=df_mixed_lm[\"participant\"], vc_formula=vc,\n",
    "                        ).fit(reml= False)\n",
    "        print(model.summary())\n",
    "        result_M11.extend(extraction_values_based_of_summary_fixed_effects(model, result_M11 ,formel, \"M11_EDA_RISK_VOL\", RISK))\n",
    "\n",
    "        formel = f\"{eda} ~ C({RISK}) * C(Vol_Classification_Global) * C(transaction) * C(window_type)\"\n",
    "\n",
    "        print(f\"\\n\\n\\n============================================= {formel} =============================================\\n\")\n",
    "        vc = {\"session\": \"0 + C(session)\"}  \n",
    "        model = smf.mixedlm(formel,\n",
    "                            data=df_mixed_lm,\n",
    "                            groups=df_mixed_lm[\"participant\"], vc_formula=vc,\n",
    "                        ).fit(reml= False)\n",
    "        print(model.summary())\n",
    "        result_M11.extend(extraction_values_based_of_summary_fixed_effects(model, result_M11 ,formel, \"M12_EDA_RISK_VOL_TRAN_WIND\", RISK))\n",
    "\n",
    "\n",
    "\n",
    "df_summary_M11 = pd.DataFrame(result_M11)  \n",
    "df_summary_M11[\"term\"] = df_summary_M11[\"term\"].apply(shorten_variable) \n",
    "df_summary_M11[\"formula\"] = df_summary_M11[\"formula\"].apply(shorten_variable)  \n",
    "df_significant_M11 = extraction_significant_results(df_summary_M11, alpha=0.05, only_main_effects=True)\n",
    "x_M11 = df_significant_M11[[\"formula\",\"term\", \"coef\", \"std_err\", \"p_value\"]].round(3)\n",
    "path = TESTS_RESULT / \"Modell_Risk_VOL_FV_Results\"\n",
    "saving_matrix_csv_latex(x_M11, file_name=path, caption = \"LMM - EDA ~¬†Risk * VOL Categorization\", label = \"tab:LMM_RISK_VOL_Categorization\", FOLDER = TESTS_RESULT)\n",
    "\n",
    "df_significant_M11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44adebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_significant_M11[[\"formula\",\"term\", \"coef\", \"std_err\", \"p_value\"]].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f7205",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "This chapter applies a Random Forest Classifier with a training split ratio of 70:30. The goal is to predict the risky, medium risk, and low (no) risk categories based on EDA. First, the feature set starts with only EDA variables. Then, trading variables are added, and finally, market mechanism values (RD Kirchler, V_t) are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d03fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = df_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6844f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction[\"Risk_Kirchler\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca2115",
   "metadata": {},
   "source": [
    "### Model: Predict Volatility Classification with EDA signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature set\n",
    "features = [\"scl_mean\", \"scl_std\", \"scr_amplitude\", \"scr_auc\", \"window_type\", \"session\"]\n",
    "target = [\"Vol_Classification_Global\"]          #target\n",
    "mapping = {0.0: 0, 0.5: 1, 1.0: 2}\n",
    "df_target = df_prediction[target].iloc[:, 0].map(mapping).astype(int).copy()        #mapping the catgories of risk to intgers (0 (Lower Risk) = 0, 0.5 (Medium-Risk) -> 1, 1 (Risky) -> 2)\n",
    "df_target\n",
    "\n",
    "df_features =df_prediction[features].copy()\n",
    "# One-Hot Encoder for the window-type and session (e.g., pre-decision -> gets its own column. All session of this )\n",
    "df_features = pd.get_dummies(df_features, \n",
    "                       columns=[\"window_type\", \"session\"], \n",
    "                       drop_first=True)\n",
    "\n",
    "# Splitting the data in training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features,df_target, test_size= 0.3, random_state=42, stratify = df_target)\n",
    "\n",
    "# create and fit model\n",
    "model_rdf = RandomForestClassifier(n_estimators = 800, random_state=42)\n",
    "model_rdf.fit(X_train, y_train)\n",
    "\n",
    "# Predcition of the test sample\n",
    "y_pred = model_rdf.predict(X_test)\n",
    "y_prob = model_rdf.predict_proba(X_test)\n",
    "\n",
    "#Calculating AUC, Accuracy and the classification report\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_prob, multi_class=\"ovr\", average = \"macro\")}\")\n",
    "class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "# saving classifcation report as latex table\n",
    "print(class_report)\n",
    "df_class = pd.DataFrame(class_report).transpose()\n",
    "path = TESTS_RESULT / \"RFC_CLass_report_V_t_EDA\"\n",
    "#saving_matrix_csv_latex(class_report, file_name=path, caption = \"Random Foreest Classifer - Classification Report of Risk based on EDA and trading variables\", label = \"tab:RFC_EDA_Giusti_TV\", FOLDER = TESTS_RESULT)\n",
    "latex = df_class.to_latex(  float_format=\"%.3f\", index=True, bold_rows=True, column_format=\"lcccc\", caption=\"Classification Report\",  label=\"tab:classification_report\")\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(latex)\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "\n",
    "\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Creating confusion matrix \n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "\n",
    "# Saving confusion matrix as a figure\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "disp.plot()\n",
    "path = TESTS_RESULT / \"RFC_CM_V_t_RISK.png\"\n",
    "plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "# Printing the feature importance of each feature\n",
    "print(\"=============== Featuere Importance ===============================\")\n",
    "importances = model_rdf.feature_importances_\n",
    "\n",
    "for name, imp in sorted(zip(df_features.columns, importances), key=lambda x: -x[1]):\n",
    "    print(f\"{name}: {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ccaca",
   "metadata": {},
   "source": [
    "### Model: Predict Riks Kirchler Classification with EDA signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"scl_mean\", \"scl_std\", \"scr_amplitude\", \"scr_auc\", \"window_type\", \"session\"]\n",
    "target = [\"Risk_Kirchler\"]\n",
    "mapping = {0.0: 0, 0.5: 1, 1.0: 2}\n",
    "df_target = df_prediction[target].iloc[:, 0].map(mapping).astype(int).copy()\n",
    "df_target\n",
    "\n",
    "# One hot encoder\n",
    "df_features =df_prediction[features].copy()\n",
    "df_features = pd.get_dummies(df_features, \n",
    "                       columns=[\"window_type\", \"session\"], \n",
    "                       drop_first=True)\n",
    "\n",
    "# Splitting datset: 70: 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features,df_target, test_size= 0.3, random_state=42, stratify = df_target)\n",
    "\n",
    "#Create and fit model\n",
    "model_rdf = RandomForestClassifier(n_estimators = 800, random_state=42)\n",
    "model_rdf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test sample\n",
    "y_pred = model_rdf.predict(X_test)\n",
    "y_prob = model_rdf.predict_proba(X_test)\n",
    "\n",
    "#Creating and saving assessment metrics in latex\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_prob, multi_class=\"ovr\", average = \"macro\")}\")\n",
    "class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(class_report)\n",
    "df_class = pd.DataFrame(class_report).transpose()\n",
    "path = TESTS_RESULT / \"RFC_CLass_report_Risk_Kirchler_EDA\"\n",
    "#saving_matrix_csv_latex(class_report, file_name=path, caption = \"Random Foreest Classifer - Classification Report of Risk based on EDA and trading variables\", label = \"tab:RFC_EDA_Giusti_TV\", FOLDER = TESTS_RESULT)\n",
    "latex = df_class.to_latex(  float_format=\"%.3f\", index=True, bold_rows=True, column_format=\"lcccc\", caption=\"Classification Report\",  label=\"tab:classification_report\")\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(latex)\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "\n",
    "\n",
    "print(y_test.value_counts())\n",
    "\n",
    "#Create confusion matrix and save it as a figure\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "disp.plot()\n",
    "path = TESTS_RESULT / \"RFC_CM_EDA_RISK.png\"\n",
    "plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "# Printing the feature importance of each variable\n",
    "print(\"=============== Feature Importance ===============================\")\n",
    "importances = model_rdf.feature_importances_\n",
    "\n",
    "for name, imp in sorted(zip(df_features.columns, importances), key=lambda x: -x[1]):\n",
    "    print(f\"{name}: {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ceca0",
   "metadata": {},
   "source": [
    "### Model: Predict Risk Kirchler with EDA variables and transaction variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new feature set = EDA + Trading variables\n",
    "\n",
    "features = [\"scl_mean\", \"scl_std\", \"scr_amplitude\", \"scr_auc\",\"price\", \"transaction\", \"window_type\", \"session\"]\n",
    "target = [\"Risk_Kirchler\"]\n",
    "mapping = {0.0: 0, 0.5: 1, 1.0: 2}\n",
    "df_target = df_prediction[target].iloc[:, 0].map(mapping).astype(int).copy()\n",
    "df_target\n",
    "# one hot encoder for transaction, window_tyoe, session\n",
    "df_features =df_prediction[features].copy()\n",
    "df_features = pd.get_dummies(df_features, \n",
    "                       columns=[\"transaction\", \"window_type\", \"session\"], \n",
    "                       drop_first=True)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features,df_target, test_size= 0.3, random_state=42, stratify = df_target)\n",
    "\n",
    "model_rdf = RandomForestClassifier(n_estimators = 800, random_state=42)\n",
    "model_rdf.fit(X_train, y_train)\n",
    "\n",
    "#predict test output\n",
    "y_pred = model_rdf.predict(X_test)\n",
    "y_prob = model_rdf.predict_proba(X_test)\n",
    "\n",
    "#Assess prediction and saving it as a LaTex file\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_prob, multi_class=\"ovr\", average = \"macro\")}\")\n",
    "class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(class_report)\n",
    "df_class = pd.DataFrame(class_report).transpose()\n",
    "path = TESTS_RESULT / \"RFC_CLass_report_Risk_Kirchler_EDA_TV\"\n",
    "#saving_matrix_csv_latex(class_report, file_name=path, caption = \"Random Foreest Classifer - Classification Report of Risk based on EDA and trading variables\", label = \"tab:RFC_EDA_Giusti_TV\", FOLDER = TESTS_RESULT)\n",
    "latex = df_class.to_latex(  float_format=\"%.3f\", index=True, bold_rows=True, column_format=\"lcccc\", caption=\"Classification Report\",  label=\"tab:classification_report\")\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(latex)\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "\n",
    "\n",
    "print(y_test.value_counts())\n",
    "\n",
    "#Create Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "disp.plot()\n",
    "path = TESTS_RESULT / \"RFC_CM_EDA_RISK_K_TV.png\"\n",
    "plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "#Return feature importance\n",
    "print(\"=============== Featuere Importance ===============================\")\n",
    "\n",
    "importances = model_rdf.feature_importances_\n",
    "\n",
    "for name, imp in sorted(zip(df_features.columns, importances), key=lambda x: -x[1]):\n",
    "    print(f\"{name}: {imp:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f163c",
   "metadata": {},
   "source": [
    "### Model: Predict Volatility Classification based on EDA and trading variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a18bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature = EDA + trading variables\n",
    "df_prediction = df_all.copy()\n",
    "df_prediction = df_prediction.dropna(subset=[\"Vol_Classification_Global\"])\n",
    "features = [\"scl_mean\", \"scl_std\", \"scr_amplitude\", \"scr_auc\", \"price\", \"transaction\", \"window_type\", \"session\"]\n",
    "target = [\"Vol_Classification_Global\"]\n",
    "mapping = {0.0: 0, 0.5: 1, 1.0: 2} #transform of target values\n",
    "df_target = df_prediction[target].iloc[:, 0].map(mapping).astype(int).copy()\n",
    "df_target\n",
    "#One Hot Encoder\n",
    "df_features =df_prediction[features].copy()\n",
    "df_features = pd.get_dummies(df_features, \n",
    "                       columns=[\"transaction\", \"window_type\", \"session\"], \n",
    "                       drop_first=True)\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features,df_target, test_size= 0.3, random_state=42, stratify = df_target)\n",
    "\n",
    "#Create Classifier and fit the classifer\n",
    "model_rdf = RandomForestClassifier(n_estimators = 800, random_state=42)\n",
    "model_rdf.fit(X_train, y_train)\n",
    "\n",
    "# Predict test dataset\n",
    "y_pred = model_rdf.predict(X_test)\n",
    "y_prob = model_rdf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_prob, multi_class=\"ovr\", average = \"macro\")}\")\n",
    "cp = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(cp)\n",
    "df_class = pd.DataFrame(cp).transpose()\n",
    "path = TESTS_RESULT / \"RFC_CLass_report_Risk_Vol_EDA_TV\"\n",
    "#saving_matrix_csv_latex(class_report, file_name=path, caption = \"Random Foreest Classifer - Classification Report of Risk based on EDA and trading variables\", label = \"tab:RFC_EDA_Giusti_TV\", FOLDER = TESTS_RESULT)\n",
    "latex = df_class.to_latex(  float_format=\"%.3f\", index=True, bold_rows=True, column_format=\"lcccc\", caption=\"Classification Report Vol\",  label=\"tab:classification_report_Vol\")\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(latex)\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "disp.plot()\n",
    "path = TESTS_RESULT / \"RFC_CM_EDA_RISK_Vol_TV.png\"\n",
    "plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "\n",
    "print(\"=============== Featuere Importance ===============================\")\n",
    "importances = model_rdf.feature_importances_\n",
    "\n",
    "for name, imp in sorted(zip(df_features.columns, importances), key=lambda x: -x[1]):\n",
    "    print(f\"{name}: {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174b8e5",
   "metadata": {},
   "source": [
    "### Model: Predict Volatility Classification with Market Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7dd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = df_all.copy()\n",
    "df_prediction = df_prediction.dropna(subset=[\"Vol_Classification_Global\"])\n",
    "# new feature set = EDA + trading variables + market mechanism (RD, V_t)\n",
    "features = [\"scl_mean\", \"scl_std\", \"scr_amplitude\", \"scr_auc\", \"price\", \"transaction\", \"window_type\", \"session\", \"rd_Kirchler_per_round\", \"V_t\"]\n",
    "target = [\"Vol_Classification_Global\"]\n",
    "mapping = {0.0: 0, 0.5: 1, 1.0: 2}\n",
    "df_target = df_prediction[target].iloc[:, 0].map(mapping).astype(int).copy()\n",
    "df_target\n",
    "\n",
    "df_features =df_prediction[features].copy()\n",
    "df_features = pd.get_dummies(df_features, \n",
    "                       columns=[\"transaction\", \"window_type\", \"session\"], \n",
    "                       drop_first=True)\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features,df_target, test_size= 0.3, random_state=42, stratify = df_target)\n",
    "\n",
    "# create RF classifier and fit the classifier\n",
    "model_rdf = RandomForestClassifier(n_estimators = 800, random_state=42)\n",
    "model_rdf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model_rdf.predict(X_test)\n",
    "y_prob = model_rdf.predict_proba(X_test)\n",
    "\n",
    "# Assessment of the prediction\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_prob, multi_class=\"ovr\", average = \"macro\")}\")\n",
    "cp = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(cp)\n",
    "df_class = pd.DataFrame(cp).transpose()\n",
    "path = TESTS_RESULT / \"RFC_CLass_report_Vol_with_V_TV.png\"\n",
    "#saving_matrix_csv_latex(class_report, file_name=path, caption = \"Random Foreest Classifer - Classification Report of Risk based on EDA and trading variables\", label = \"tab:RFC_EDA_Giusti_TV\", FOLDER = TESTS_RESULT)\n",
    "latex = df_class.to_latex(  float_format=\"%.3f\", index=True, bold_rows=True, column_format=\"lcccc\", caption=\"Classification Report Vol\",  label=\"tab:classification_report_Vol\")\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(latex)\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "\n",
    "print(y_test.value_counts())\n",
    "\n",
    "#Confusuon Matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "disp.plot()\n",
    "path = TESTS_RESULT / \"RFC_CM_EDA_RISK_Vol_with_V_TV.png\"\n",
    "plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "print(\"=============== Featuere Importance ===============================\")\n",
    "importances = model_rdf.feature_importances_\n",
    "\n",
    "for name, imp in sorted(zip(df_features.columns, importances), key=lambda x: -x[1]):\n",
    "    print(f\"{name}: {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056060f2",
   "metadata": {},
   "source": [
    "### Model: Predict Volatility Classification based on EDA + RD + trading variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffeb5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = df_all.copy()\n",
    "df_prediction = df_prediction.dropna(subset=[\"Vol_Classification_Global\"])\n",
    "# new feature = EDA + trading variables + RD\n",
    "features = [\"scl_mean\", \"scl_std\", \"scr_amplitude\", \"scr_auc\", \"price\", \"transaction\", \"window_type\", \"session\", \"rd_Kirchler_per_round\"]\n",
    "target = [\"Vol_Classification_Global\"]\n",
    "mapping = {0.0: 0, 0.5: 1, 1.0: 2}\n",
    "df_target = df_prediction[target].iloc[:, 0].map(mapping).astype(int).copy()\n",
    "df_target\n",
    "\n",
    "df_features =df_prediction[features].copy()\n",
    "df_features = pd.get_dummies(df_features, \n",
    "                       columns=[\"transaction\", \"window_type\", \"session\"], \n",
    "                       drop_first=True)\n",
    "\n",
    "#split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features,df_target, test_size= 0.3, random_state=42, stratify = df_target)\n",
    "# Create and fit\n",
    "model_rdf = RandomForestClassifier(n_estimators = 800, random_state=42)\n",
    "model_rdf.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred = model_rdf.predict(X_test)\n",
    "y_prob = model_rdf.predict_proba(X_test)\n",
    "# Report\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_prob, multi_class=\"ovr\", average = \"macro\")}\")\n",
    "cp = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(cp)\n",
    "df_class = pd.DataFrame(cp).transpose()\n",
    "path = TESTS_RESULT / \"RFC_CLass_report_Risk_with_riskK_TV\"\n",
    "#saving_matrix_csv_latex(class_report, file_name=path, caption = \"Random Foreest Classifer - Classification Report of Risk based on EDA and trading variables\", label = \"tab:RFC_EDA_Giusti_TV\", FOLDER = TESTS_RESULT)\n",
    "latex = df_class.to_latex(  float_format=\"%.3f\", index=True, bold_rows=True, column_format=\"lcccc\", caption=\"Classification Report Vol\",  label=\"tab:classification_report_Vol\")\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(latex)\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "\n",
    "print(y_test.value_counts())\n",
    "\n",
    "#Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "disp.plot()\n",
    "path = TESTS_RESULT / \"RFC_CM_EDA_RISK_Vol__with_riskK_TV.png\"\n",
    "plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "print(\"=============== Featuere Importance ===============================\")\n",
    "importances = model_rdf.feature_importances_\n",
    "\n",
    "for name, imp in sorted(zip(df_features.columns, importances), key=lambda x: -x[1]):\n",
    "    print(f\"{name}: {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc12fd0",
   "metadata": {},
   "source": [
    "### Model: Predict RD Classification based on EDA + trading variables + FV-Kirchler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = df_full.copy()\n",
    "df_prediction = df_prediction.dropna(subset=[\"Vol_Classification_Global\"])\n",
    "features = [\"scl_mean\", \"scl_std\", \"scr_amplitude\", \"scr_auc\", \"price\", \"transaction\", \"window_type\", \"session\", \"FV_KIRCHLER\"]\n",
    "target = [\"Risk_Kirchler\"]\n",
    "mapping = {0.0: 0, 0.5: 1, 1.0: 2}\n",
    "df_target = df_prediction[target].iloc[:, 0].map(mapping).astype(int).copy()\n",
    "df_target\n",
    "\n",
    "df_features =df_prediction[features].copy()\n",
    "df_features = pd.get_dummies(df_features, \n",
    "                       columns=[\"transaction\", \"window_type\", \"session\"], \n",
    "                       drop_first=True)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features,df_target, test_size= 0.3, random_state=42, stratify = df_target)\n",
    "\n",
    "model_rdf = RandomForestClassifier(n_estimators = 800, random_state=42)\n",
    "model_rdf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model_rdf.predict(X_test)\n",
    "y_prob = model_rdf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_prob, multi_class=\"ovr\", average = \"macro\")}\")\n",
    "cp = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(cp)\n",
    "df_class = pd.DataFrame(cp).transpose()\n",
    "path = TESTS_RESULT / \"RFC_CLass_report_Risk_K_with_K_TV\"\n",
    "#saving_matrix_csv_latex(class_report, file_name=path, caption = \"Random Foreest Classifer - Classification Report of Risk based on EDA and trading variables\", label = \"tab:RFC_EDA_Giusti_TV\", FOLDER = TESTS_RESULT)\n",
    "latex = df_class.to_latex(  float_format=\"%.3f\", index=True, bold_rows=True, column_format=\"lcccc\", caption=\"Classification Report Vol\",  label=\"tab:classification_report_Vol\")\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(latex)\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "\n",
    "print(y_test.value_counts())\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "disp.plot()\n",
    "path = TESTS_RESULT / \"RFC_CM_EDA_RISK_K_with_K.png\"\n",
    "plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "print(\"=============== Featuere Importance ===============================\")\n",
    "importances = model_rdf.feature_importances_\n",
    "\n",
    "for name, imp in sorted(zip(df_features.columns, importances), key=lambda x: -x[1]):\n",
    "    print(f\"{name}: {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efd83fd",
   "metadata": {},
   "source": [
    "### Predict Risk Kirchler Classification with EDA + trading variables + V_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = df_full.copy()\n",
    "df_prediction = df_prediction.dropna(subset=[\"Vol_Classification_Global\"])\n",
    "features = [\"scl_mean\", \"scl_std\", \"scr_amplitude\", \"scr_auc\", \"price\", \"transaction\", \"window_type\", \"session\", \"V_t\"]\n",
    "target = [\"Risk_Kirchler\"]\n",
    "mapping = {0.0: 0, 0.5: 1, 1.0: 2}\n",
    "df_target = df_prediction[target].iloc[:, 0].map(mapping).astype(int).copy()\n",
    "df_target\n",
    "# One Hot Encoder\n",
    "df_features =df_prediction[features].copy()\n",
    "df_features = pd.get_dummies(df_features, \n",
    "                       columns=[\"transaction\", \"window_type\", \"session\"], \n",
    "                       drop_first=True)\n",
    "\n",
    "#split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features,df_target, test_size= 0.3, random_state=42, stratify = df_target)\n",
    "#create and fit classifier\n",
    "model_rdf = RandomForestClassifier(n_estimators = 800, random_state=42)\n",
    "model_rdf.fit(X_train, y_train)\n",
    "\n",
    "#predict\n",
    "y_pred = model_rdf.predict(X_test)\n",
    "y_prob = model_rdf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_prob, multi_class=\"ovr\", average = \"macro\")}\")\n",
    "cp = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(cp)\n",
    "df_class = pd.DataFrame(cp).transpose()\n",
    "path = TESTS_RESULT / \"RFC_CLass_report_risk_K_wih_V_TV\"\n",
    "#saving_matrix_csv_latex(class_report, file_name=path, caption = \"Random Foreest Classifer - Classification Report of Risk based on EDA and trading variables\", label = \"tab:RFC_EDA_Giusti_TV\", FOLDER = TESTS_RESULT)\n",
    "latex = df_class.to_latex(  float_format=\"%.3f\", index=True, bold_rows=True, column_format=\"lcccc\", caption=\"Classification Report Vol\",  label=\"tab:classification_report_Vol\")\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(latex)\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "\n",
    "print(y_test.value_counts())\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "disp.plot()\n",
    "path = TESTS_RESULT / \"RFC_CM_EDA_RISK_K_wih_V_TV.png\"\n",
    "plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "if path.exists():\n",
    "    print(path)\n",
    "\n",
    "\n",
    "print(\"=============== Featuere Importance ===============================\")\n",
    "importances = model_rdf.feature_importances_\n",
    "\n",
    "for name, imp in sorted(zip(df_features.columns, importances), key=lambda x: -x[1]):\n",
    "    print(f\"{name}: {imp:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
